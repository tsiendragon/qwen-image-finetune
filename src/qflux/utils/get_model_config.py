#!/usr/bin/env python3
"""
è·å–Qwen-Image-Editæ¨¡å‹çš„è¯¦ç»†é…ç½®å‚æ•°
"""

import json

import torch

from qflux.models.transformer_qwenimage import QwenImageTransformer2DModel


def get_pretrained_model_config():
    """è·å–é¢„è®­ç»ƒæ¨¡å‹é…ç½®"""
    print("ğŸ” æ­£åœ¨è·å–Qwen-Image-Editæ¨¡å‹é…ç½®...")

    try:
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        model = QwenImageTransformer2DModel.from_pretrained(
            "Qwen/Qwen-Image-Edit", subfolder="transformer", torch_dtype=torch.bfloat16
        )

        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")

        # è·å–é…ç½®
        config = model.config

        print("\nğŸ“‹ æ¨¡å‹é…ç½®å‚æ•°ï¼š")
        print("=" * 60)

        # æ‰“å°æ‰€æœ‰é…ç½®å‚æ•°
        if hasattr(config, "__dict__"):
            config_dict = config.__dict__
        else:
            config_dict = dict(config)

        for key, value in sorted(config_dict.items()):
            if not key.startswith("_"):
                print(f"{key:25}: {value}")

        # è®¡ç®—å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

        print("\nğŸ“Š å‚æ•°ç»Ÿè®¡ï¼š")
        print("=" * 60)
        print(f"{'æ€»å‚æ•°é‡':25}: {total_params:,}")
        print(f"{'å¯è®­ç»ƒå‚æ•°é‡':25}: {trainable_params:,}")
        print(f"{'æ¨¡å‹å¤§å° (MB)':25}: {total_params * 2 / 1024 / 1024:.2f}")  # bfloat16 = 2 bytes

        # ä¿å­˜é…ç½®åˆ°JSONæ–‡ä»¶
        config_for_save = {}
        for key, value in config_dict.items():
            if not key.startswith("_"):
                # è½¬æ¢ä¸ºJSONå…¼å®¹çš„æ ¼å¼
                if isinstance(value, torch.dtype):
                    config_for_save[key] = str(value)
                elif hasattr(value, "__dict__"):
                    config_for_save[key] = str(value)
                else:
                    config_for_save[key] = value

        with open("qwen_image_edit_config.json", "w", encoding="utf-8") as f:
            json.dump(config_for_save, f, indent=2, ensure_ascii=False)

        print("\nğŸ’¾ é…ç½®å·²ä¿å­˜åˆ°: qwen_image_edit_config.json")

        # æ‰“å°å…³é”®çš„transformeræ¶æ„å‚æ•°
        print("\nğŸ—ï¸ å…³é”®æ¶æ„å‚æ•°ï¼š")
        print("=" * 60)
        key_params = [
            "num_layers",
            "num_attention_heads",
            "attention_head_dim",
            "in_channels",
            "out_channels",
            "patch_size",
            "sample_size",
            "hidden_size",
            "num_single_layers",
            "pooled_projection_dim",
        ]

        for param in key_params:
            if param in config_dict:
                print(f"{param:25}: {config_dict[param]}")

        return config_dict

    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½æ¨¡å‹: {e}")
        print("\nå¯èƒ½çš„åŸå› ï¼š")
        print("- ç½‘ç»œè¿æ¥é—®é¢˜")
        print("- éœ€è¦Hugging Faceè®¤è¯")
        print("- æ¨¡å‹è·¯å¾„æˆ–åç§°é”™è¯¯")
        print("- ç¼ºå°‘ä¾èµ–åŒ…")
        return None


def compare_with_local_config():
    """æ¯”è¾ƒæœ¬åœ°æ¨¡å‹é…ç½®"""
    print("\nğŸ”„ æ¯”è¾ƒæœ¬åœ°æ¨¡å‹é…ç½®...")

    # æœ¬åœ°æ¨¡å‹é…ç½®
    local_config = {
        "patch_size": 2,
        "in_channels": 64,
        "out_channels": 16,
        "num_layers": 60,
        "attention_head_dim": 128,
        "num_attention_heads": 24,
    }

    print("\nğŸ“ æœ¬åœ°æ¨¡å‹é…ç½®ï¼š")
    print("=" * 60)
    for key, value in local_config.items():
        print(f"{key:25}: {value}")

    # åˆ›å»ºæœ¬åœ°æ¨¡å‹å¹¶è®¡ç®—å‚æ•°
    try:
        local_model = QwenImageTransformer2DModel(**local_config)
        local_params = sum(p.numel() for p in local_model.parameters())
        print(f"{'æœ¬åœ°æ¨¡å‹å‚æ•°é‡':25}: {local_params:,}")
    except Exception as e:
        print(f"âŒ åˆ›å»ºæœ¬åœ°æ¨¡å‹å¤±è´¥: {e}")


if __name__ == "__main__":
    # è·å–é¢„è®­ç»ƒæ¨¡å‹é…ç½®
    pretrained_config = get_pretrained_model_config()

    # æ¯”è¾ƒæœ¬åœ°é…ç½®
    compare_with_local_config()

    print("\nğŸ é…ç½®è·å–å®Œæˆï¼")
