trainer: FluxKontext
mode: predict
model:
  pretrained_model_name_or_path: black-forest-labs/FLUX.1-Kontext-dev
  lora:
    r: 16
    lora_alpha: 16
    init_lora_weights: gaussian
    target_modules:
    - to_k
    - to_q
    - to_v
    - to_out.0
    adapter_name: lora_edit
  quantize: true
data:
  class_path: src.data.dataset.ImageDataset
  init_args:
    dataset_path:
    - split: test
      repo_id: TsienDragon/face_segmentation_20
    caption_dropout_rate: 0.1
    prompt_image_dropout_rate: 0.1
    cache_dir: /raid/lilong/data/experiment/flux-kontext-ktp_lora_fp16/cache
    use_cache: true
    use_edit_mask: true
    selected_control_indexes:
    - 1
    prompt_empty_drop_keys:
    - prompt_embed
    - pooled_prompt_embed
    processor:
      class_path: src.data.preprocess.ImageProcessor
      init_args:
        process_type: resize
        resize_mode: bilinear
        target_size:
        - 832
        - 576
        controls_size:
        - - 832
          - 576
  batch_size: 8
  num_workers: 2
  shuffle: true
logging:
  output_dir: /raid/lilong/data/experiment/flux-kontext-ktp_lora_fp16
  report_to: tensorboard
  tracker_project_name: ktp_inpaint
  sampling:
    enable: false
    validation_steps: 100
    num_samples: 1
    seed: 42
optimizer:
  class_path: bitsandbytes.optim.Adam8bit
  init_args:
    lr: 0.0001
    betas:
    - 0.9
    - 0.999
lr_scheduler:
  scheduler_type: cosine
  warmup_steps: 50
  num_cycles: 0.5
  power: 1.0
train:
  train_batch_size: 8
  gradient_accumulation_steps: 1
  max_train_steps: 10000
  num_epochs: 3
  checkpointing_steps: 100
  max_grad_norm: 1.0
  mixed_precision: bf16
  gradient_checkpointing: true
  low_memory: true
  fit_device:
    vae_encoder: cuda:1
    text_encoder: cuda:1
cache:
  devices:
    vae_encoder: cuda:1
    text_encoder: cuda:0
    text_encoder_2: cuda:0
  use_cache: true
  cache_dir: /raid/lilong/data/experiment/flux-kontext-ktp_lora_fp16/cache
  prompt_empty_drop_keys:
  - prompt_embed
  - pooled_prompt_embed
predict:
  devices:
    vae: cuda:1
    text_encoder: cuda:1
    text_encoder_2: cuda:1
    dit: cuda:1
loss:
  mask_loss: false
  forground_weight: 2.0
  background_weight: 1.0
trainer_type: FluxKontext
use_cache: true
cache_dir: /raid/lilong/data/experiment/flux-kontext-ktp_lora_fp16/cache
model_name: black-forest-labs/FLUX.1-Kontext-dev
lora_adapter_name: lora_edit
lora_r: 16
lora_lora_alpha: 16
target_size:
- 832
- 576
caption_dropout_rate: 0.1
quantization_type: fp8_online
