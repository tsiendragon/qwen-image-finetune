# v3.1.0 Release Notes

## Overview

Version 3.1.0 introduces two major features: a unified multi-logger interface supporting TensorBoard, Weights & Biases, and SwanLab, plus enhanced validation sampling during training with improved visualization capabilities. These features enable flexible experiment tracking and real-time monitoring of generation quality.

## New Features

### Multi-Logger Support

- **Unified Interface**: Single `LoggerManager` API works with TensorBoard, Weights & Biases (wandb), and SwanLab
- **Easy Switching**: Change logging backends by modifying one config parameter (`report_to`)
- **Automatic Authentication**: API keys loaded from `.env` file with automatic login
- **Distributed Training**: Automatic process coordination, only main process logs metrics
- **Rich Logging**: Support for scalars, images, text, and tables across all backends

#### Configuration

```yaml
logging:
  output_dir: "./output/experiment"
  report_to: "tensorboard"  # or "wandb" or "swanlab"
  tracker_project_name: "my-project"
  tags:
    - experiment
    - lora
  notes: "Experiment description"
```

#### Authentication

For wandb/swanlab, create `.env` file:
```bash
WANDB_API_KEY=your_api_key
SWANLAB_API_KEY=your_api_key
```

See [Logging Guide](../guide/logging.md) for detailed usage.

### Validation Sampling During Training

- **Real-time visualization**: Periodically sample from validation data during training to visualize generation quality in TensorBoard
- **Multiple data sources**: Support for validation data from HuggingFace datasets, CSV files, or direct configuration
- **Configurable sampling**: Control validation frequency, sample count, and optional fixed seed for reproducible comparisons
- **Memory-efficient**: Store validation embeddings on CPU and only move to GPU when needed
- **Distributed-friendly**: Works with multi-GPU training by distributing validation samples across devices

### Configuration

Add validation configuration to your YAML config:

```yaml
validation:
  enabled: true       # Enable validation sampling
  steps: 500          # Run validation every N steps
  max_samples: 5      # Maximum number of validation samples to use
  seed: 42            # Optional: Fixed seed for reproducible sampling

  # HuggingFace dataset configuration
  dataset:
    class_path: "qflux.data.datasets.HuggingFaceDataset"
    init_args:
      dataset_name: "namespace/dataset_name"
      split: "validation"
      image_column: "control_image"
      caption_column: "prompt"
      additional_control_columns:
        - "control_image_depth"
        - "control_image_canny"

  # Or use direct samples configuration
  samples:
    - prompt: "A cute cat sitting on a windowsill"
      images:
        - "path/to/cat_control.png"
        - "path/to/cat_depth.png"
        - "path/to/cat_canny.png"
      controls_size: [[512, 512], [512, 512], [512, 512]]
      height: 512
      width: 512
```

## Implementation Details

- Implemented as a mixin class that integrates with all trainer types
- Compatible with all existing trainers (QwenImageEditTrainer, QwenImageEditPlusTrainer, FluxKontextLoraTrainer)
- Automatically handles different parameter formats between trainer types
- Minimal memory overhead by storing embeddings on CPU and moving to GPU only during sampling
- Efficient TensorBoard logging with direct tensor visualization

## Future Extensions

- Add quantitative metrics for validation (FID, CLIP score)
- Support for custom validation callbacks
- Enable validation image export to disk
