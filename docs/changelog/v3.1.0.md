# v3.1.0 Release Notes

## Overview

Version 3.1.0 introduces validation sampling during training, allowing real-time monitoring of generation quality through TensorBoard visualizations. This feature helps track model progress, identify issues early, and better tune hyperparameters with immediate visual feedback.

## New Features

### Validation Sampling During Training

- **Real-time visualization**: Periodically sample from validation data during training to visualize generation quality in TensorBoard
- **Multiple data sources**: Support for validation data from HuggingFace datasets, CSV files, or direct configuration
- **Configurable sampling**: Control validation frequency, sample count, and optional fixed seed for reproducible comparisons
- **Memory-efficient**: Store validation embeddings on CPU and only move to GPU when needed
- **Distributed-friendly**: Works with multi-GPU training by distributing validation samples across devices

### Configuration

Add validation configuration to your YAML config:

```yaml
validation:
  enabled: true       # Enable validation sampling
  steps: 500          # Run validation every N steps
  max_samples: 5      # Maximum number of validation samples to use
  seed: 42            # Optional: Fixed seed for reproducible sampling

  # HuggingFace dataset configuration
  dataset:
    class_path: "qflux.data.datasets.HuggingFaceDataset"
    init_args:
      dataset_name: "namespace/dataset_name"
      split: "validation"
      image_column: "control_image"
      caption_column: "prompt"
      additional_control_columns:
        - "control_image_depth"
        - "control_image_canny"

  # Or use direct samples configuration
  samples:
    - prompt: "A cute cat sitting on a windowsill"
      images:
        - "path/to/cat_control.png"
        - "path/to/cat_depth.png"
        - "path/to/cat_canny.png"
      controls_size: [[512, 512], [512, 512], [512, 512]]
      height: 512
      width: 512
```

## Implementation Details

- Implemented as a mixin class that integrates with all trainer types
- Compatible with all existing trainers (QwenImageEditTrainer, QwenImageEditPlusTrainer, FluxKontextLoraTrainer)
- Automatically handles different parameter formats between trainer types
- Minimal memory overhead by storing embeddings on CPU and moving to GPU only during sampling
- Efficient TensorBoard logging with direct tensor visualization

## Future Extensions

- Add quantitative metrics for validation (FID, CLIP score)
- Support for custom validation callbacks
- Enable validation image export to disk
