# v1.6.0 sampling during train

To better monitor the training process, it is better to have a sampling data visualized in tensorboard. Otherwise, it is time-cosuming to debug.
Of course, if the training works very well, we can disable the sampling function to speed up the training.

##  Functionality

1. Define a validation loop during the training. In the validation loop. Do the sampling
2. To speed up, we could first calculated the vae latent and prompt embedding in the initial stage of the training step
3. We only need to move vae decoder to the gpu for the sampling
4. log one or several sampling images (better to have prompt disapled)
5. log one or several decoded imgae latent and control latent to the tensorboard to verify the vae encoding is correct
6. log one or several target image and control image in the tensorboard to verify the the input is correct. If possible, aslo display the input prompt
7. When log the image in the tensorboard, resize the image such that it longest size is no more than 512 to save calculation
8. After the validation loop ended, move the vae decoder to cpu to save memory
9. If using multiple gpu DDP training, do the log in each process. If we aim to log N images. Then each process log max(1,N//num_process) images
10. If the validation dataset len is less than num_process, repeat it until it match the length

## Configuration Parameters

The sampling functionality should be configurable through the logging section of the YAML configuration file with minimal, essential parameters:

```yaml
logging:
  output_dir: "/path/to/output"
  logging_dir: "logs"
  report_to: "tensorboard"
  tracker_project_name: "project_name"

  # Sampling during training configuration
  sampling:
    enable: true              # Enable/disable sampling functionality
    validation_steps: 100     # Run validation sampling every N training steps
    num_samples: 4           # Number of samples to generate and log per validation
    seed: 42                 # Seed for reproducible sampling

    # Validation data configuration
    validation_data: null     # Path to validation dataset or list of control-prompt pairs
    # Examples:
    # validation_data: "/path/to/validation/dataset"  # Single dataset path
    # validation_data:                                # List of control-prompt pairs
    #   - control: "/path/to/control1.jpg"
    #     prompt: "A beautiful landscape"
    #   - control: "/path/to/control2.jpg"
    #     prompt: "A modern building"
```

## Implementation Notes

### Default Behavior and Internal Configuration
- **Dataset**: Use training dataset subset if no validation_data specified
- **Validation Data**: Support both dataset path and control-prompt pairs list
- **Image Processing**: Automatically resize images to max 512px for tensorboard logging
- **Memory Management**: Automatically move VAE decoder between GPU/CPU for optimal memory usage
- **Error Handling**: Skip sampling on errors without interrupting training
- **Multi-GPU**: Automatically distribute samples across processes (min 1 per process)
- **Logging**: Log generated images, target images, control images, and prompts to tensorboard
- **Device Management**: Pre-compute embeddings when possible, optimize VAE device placement
- **Embedding Caching**: Cache VAE and text embeddings at training start, reuse during validation
- **Memory Efficiency**: Only move encoders to GPU during initial caching phase

### Validation Data Format Support
- **Path Format**: Single string path to validation dataset directory
- **List Format**: List of dictionaries with `control` and `prompt` keys
- **Automatic Detection**: System automatically detects format and handles accordingly
- **Fallback**: If validation_data is null, use subset of training dataset

### Tensorboard Organization
All logs are organized under the `validation/` prefix:
- Generated images: `validation/generated/step_{step}/`
- Target images: `validation/targets/step_{step}/`
- Control images: `validation/controls/step_{step}/`
- Prompts: `validation/prompts/step_{step}/`

## Code Implementation Design

### Overview
The sampling functionality will be implemented as a **简单的工具函数**，直接在trainer的fit方法中调用，无需mixin或复杂的继承结构。这种方法最简洁、最直接。

### Architecture Design - 工具函数模式

#### 1. 核心采样器组件

**Validation Sampler**
```python
# src/validation/validation_sampler.py
class ValidationSampler:
    """Universal validation sampler that works with any model architecture"""

    def __init__(self, config, accelerator, weight_dtype):
        self.config = config
        self.accelerator = accelerator
        self.weight_dtype = weight_dtype
        self.validation_dataset = None

        # Cached embeddings storage
        self.cached_embeddings = []  # List of cached embedding dictionaries
        self.embeddings_cached = False

        # Internal configuration with sensible defaults
        self._internal_config = {
            'max_image_size': 512,
            'log_prefix': 'validation',
            'move_vae_to_cpu_after': True,
            'skip_on_error': True,
            'min_samples_per_process': 1
        }

    def setup_validation_dataset(self, train_dataset=None):
        """Setup validation dataset from config or training dataset"""
        if self.config.validation_data is None:
            # Use subset of training dataset
            self.validation_dataset = self._create_subset_from_training(train_dataset)
        elif isinstance(self.config.validation_data, str):
            # Load from dataset path
            self.validation_dataset = self._load_dataset_from_path(self.config.validation_data)
        elif isinstance(self.config.validation_data, list):
            # Use control-prompt pairs
            self.validation_dataset = self._create_dataset_from_pairs(self.config.validation_data)
        else:
            raise ValueError(f"Unsupported validation_data format: {type(self.config.validation_data)}")

    def cache_embeddings(self, trainer, embeddings_config):
        """Cache embeddings for all validation samples using trainer's methods"""
        if self.embeddings_cached:
            return

        self.cached_embeddings = []

        try:
            for idx, sample in enumerate(self.validation_dataset):
                cached_sample = {'sample_idx': idx}

                # Cache VAE embeddings using trainer's encode methods
                if embeddings_config.get('cache_vae_embeddings', False):
                    if 'control_image' in sample:
                        # Use trainer's existing VAE encoding method
                        cached_sample['control_latents'] = trainer._encode_vae_image_for_validation(sample['control_image'])
                    if 'target_image' in sample:
                        cached_sample['target_latents'] = trainer._encode_vae_image_for_validation(sample['target_image'])

                # Cache text embeddings using trainer's encode methods
                if embeddings_config.get('cache_text_embeddings', False):
                    if 'prompt' in sample:
                        # Use trainer's existing prompt encoding method
                        cached_sample['text_embeddings'] = trainer._encode_prompt_for_validation(
                            sample['prompt'],
                            sample.get('control_image')
                        )

                # Store original sample data for logging
                cached_sample['original_sample'] = sample
                self.cached_embeddings.append(cached_sample)

        except Exception as e:
            self.accelerator.print(f"Failed to cache embeddings: {e}")
            return

        self.embeddings_cached = True

    def should_run_validation(self, global_step: int) -> bool:
        """Check if validation should run at current step"""
        if self.config.validation_steps <= 0:
            return False
        return global_step % self.config.validation_steps == 0

    def run_validation_loop(self, global_step: int, trainer):
        """Main validation loop using cached embeddings and trainer's methods"""
        if not self.embeddings_cached:
            self.accelerator.print("Warning: Embeddings not cached, skipping validation")
            return

        # Sample from cached embeddings
        num_samples = min(self.config.num_samples, len(self.cached_embeddings))
        selected_indices = self._select_sample_indices(num_samples, global_step)

        for idx in selected_indices:
            cached_sample = self.cached_embeddings[idx]

            # Generate sample using cached embeddings and trainer's model
            generated_latents = trainer._generate_latents_for_validation(cached_sample)

            # Decode latents to image using trainer's decode method
            generated_image = trainer._decode_latents_for_validation(generated_latents)

            # Log to tensorboard
            self._log_validation_sample(global_step, idx, cached_sample, generated_image, trainer)

    def _create_subset_from_training(self, train_dataset):
        """Create validation subset from training dataset"""
        pass

    def _load_dataset_from_path(self, dataset_path: str):
        """Load validation dataset from file path"""
        pass

    def _create_dataset_from_pairs(self, pairs: List[Dict]):
        """Create validation dataset from control-prompt pairs"""
        pass

    def _select_sample_indices(self, num_samples, global_step):
        """Select sample indices for validation"""
        # Use deterministic selection based on global_step and seed for reproducible results
        import random
        random.seed(self.config.seed + global_step)
        return random.sample(range(len(self.cached_embeddings)), num_samples)

    def _log_validation_sample(self, global_step, sample_idx, cached_sample, generated_image, trainer):
        """Log validation sample to tensorboard using trainer's logging capabilities"""
        try:
            # Log generated image
            trainer._log_image_to_tensorboard(
                f"validation/generated/step_{global_step}/sample_{sample_idx}",
                generated_image,
                global_step
            )

            # Log original images for comparison
            original_sample = cached_sample['original_sample']
            if 'control_image' in original_sample:
                trainer._log_image_to_tensorboard(
                    f"validation/control/step_{global_step}/sample_{sample_idx}",
                    original_sample['control_image'],
                    global_step
                )

            if 'target_image' in original_sample:
                trainer._log_image_to_tensorboard(
                    f"validation/target/step_{global_step}/sample_{sample_idx}",
                    original_sample['target_image'],
                    global_step
                )

            # Log prompt as text
            if 'prompt' in original_sample:
                trainer._log_text_to_tensorboard(
                    f"validation/prompts/step_{global_step}/sample_{sample_idx}",
                    original_sample['prompt'],
                    global_step
                )

        except Exception as e:
            self.accelerator.print(f"Failed to log validation sample {sample_idx}: {e}")
```

#### 2. 简化的Trainer集成方式

**FluxKontextLoraTrainer集成 (需要添加validation相关方法)**
```python
# src/trainer/flux_kontext_trainer.py (需要添加validation方法和修改fit方法)
class FluxKontextLoraTrainer(BaseTrainer):
    # ... 现有代码保持不变 ...

    # 新增validation相关方法
    def _encode_vae_image_for_validation(self, image):
        """Encode image for validation using existing VAE encoding logic"""
        # 复用现有的VAE编码逻辑，但适配validation格式
        if isinstance(image, str):  # 如果是路径，先加载图片
            from PIL import Image
            image = Image.open(image)

        # 使用现有的preprocess_image和encode_vae_image方法
        image_tensor = self.preprocess_image(torch.from_numpy(np.array(image)).permute(2, 0, 1).unsqueeze(0))
        return self.encode_vae_image(image_tensor)

    def _encode_prompt_for_validation(self, prompt, control_image=None):
        """Encode prompt for validation using existing dual encoder logic"""
        # 复用现有的encode_prompt方法
        pooled_prompt_embeds, prompt_embeds, text_ids = self.encode_prompt(
            prompt=prompt,
            prompt_2=prompt,
            device_text_encoder=self.accelerator.device,
            device_text_encoder_2=self.accelerator.device,
            max_sequence_length=self.max_sequence_length,
        )
        return {
            'pooled_prompt_embeds': pooled_prompt_embeds,
            'prompt_embeds': prompt_embeds,
            'text_ids': text_ids
        }

    def _generate_latents_for_validation(self, cached_sample):
        """Generate latents using cached embeddings and current model"""
        # 使用缓存的embeddings进行推理生成
        text_embeddings = cached_sample['text_embeddings']
        control_latents = cached_sample.get('control_latents')

        # 简化的推理逻辑，类似于predict方法但使用缓存的embeddings
        batch_size = 1
        height, width = 512, 512  # 固定尺寸用于validation

        # 准备latents
        latents, _, latent_ids, _ = self.prepare_latents(
            None, batch_size, 16, height, width,
            self.weight_dtype, self.accelerator.device
        )

        # 使用简化的推理步骤 (fewer steps for validation)
        num_inference_steps = 10
        timesteps = torch.linspace(1000, 0, num_inference_steps, device=self.accelerator.device)

        with torch.no_grad():
            for t in timesteps:
                # 简化的推理步骤
                latent_model_input = torch.cat([latents, control_latents], dim=1) if control_latents is not None else latents

                noise_pred = self.transformer(
                    hidden_states=latent_model_input,
                    timestep=t.expand(latents.shape[0]) / 1000,
                    guidance=torch.ones(latents.shape[0], device=self.accelerator.device),
                    pooled_projections=text_embeddings['pooled_prompt_embeds'],
                    encoder_hidden_states=text_embeddings['prompt_embeds'],
                    txt_ids=text_embeddings['text_ids'],
                    img_ids=latent_ids,
                    return_dict=False,
                )[0]

                # 简化的调度器步骤
                latents = latents - 0.1 * noise_pred  # 简化的更新规则

        return latents

    def _decode_latents_for_validation(self, latents):
        """Decode latents to image using existing VAE decoder"""
        # 复用现有的VAE解码逻辑
        latents = self._unpack_latents(latents, 512, 512, self.vae_scale_factor)
        latents = (latents / self.vae.config.scaling_factor) + self.vae.config.shift_factor

        # 临时将VAE decoder移到GPU
        original_device = self.vae.device
        self.vae.to(self.accelerator.device)

        try:
            with torch.no_grad():
                image = self.vae.decode(latents, return_dict=False)[0]
                # 转换为PIL格式
                image = self.image_processor.postprocess(image, output_type="pil")[0]
        finally:
            # 恢复VAE设备
            self.vae.to(original_device)

        return image

    def _log_image_to_tensorboard(self, tag, image, global_step):
        """Log image to tensorboard"""
        import numpy as np
        from PIL import Image as PILImage

        # 确保图像格式正确
        if isinstance(image, PILImage.Image):
            image_array = np.array(image)
        else:
            image_array = image

        # 调整尺寸以节省存储
        if max(image_array.shape[:2]) > 512:
            from PIL import Image as PILImage
            pil_image = PILImage.fromarray(image_array)
            pil_image.thumbnail((512, 512), PILImage.Resampling.LANCZOS)
            image_array = np.array(pil_image)

        # 转换为CHW格式用于tensorboard
        if len(image_array.shape) == 3:
            image_tensor = torch.from_numpy(image_array).permute(2, 0, 1).float() / 255.0
        else:
            image_tensor = torch.from_numpy(image_array).float()

        self.accelerator.log({tag: image_tensor}, step=global_step)

    def _log_text_to_tensorboard(self, tag, text, global_step):
        """Log text to tensorboard"""
        self.accelerator.log({tag: text}, step=global_step)

    def fit(self, train_dataloader):
        # ... 现有setup代码 ...
        self.setup_accelerator()
        self.load_model()
        # ... 其他现有代码 ...

        # 添加validation sampling setup (如果启用)
        validation_sampler = None
        if hasattr(self.config.logging, 'sampling') and self.config.logging.sampling.enable:
            from qflux.validation.validation_sampler import ValidationSampler

            validation_sampler = ValidationSampler(
                config=self.config.logging.sampling,
                accelerator=self.accelerator,
                weight_dtype=self.weight_dtype
            )

            # Setup validation dataset
            validation_sampler.setup_validation_dataset(train_dataloader.dataset)

            # Cache embeddings for validation using trainer methods
            embeddings_config = {
                'cache_vae_embeddings': True,      # Cache VAE latents
                'cache_text_embeddings': True,     # Cache text embeddings
            }
            validation_sampler.cache_embeddings(self, embeddings_config)

        # ... 现有训练循环 ...
        for epoch in range(start_epoch, self.config.train.num_epochs):
            for _, batch in enumerate(train_dataloader):
                # ... 现有training step代码 ...

                if self.accelerator.sync_gradients:
                    # ... 现有logging代码 ...

                    # 添加validation sampling
                    if validation_sampler and validation_sampler.should_run_validation(self.global_step):
                        try:
                            validation_sampler.run_validation_loop(
                                global_step=self.global_step,
                                trainer=self  # 传入trainer实例
                            )
                        except Exception as e:
                            self.accelerator.print(f"Validation sampling failed: {e}")
```

**QwenImageEditTrainer集成 (需要添加validation相关方法)**
```python
# src/trainer/qwen_image_edit_trainer.py (需要添加validation方法和修改fit方法)
class QwenImageEditTrainer:
    # ... 现有代码保持不变 ...

    # 新增validation相关方法
    def _encode_vae_image_for_validation(self, image):
        """Encode image for validation using existing VAE encoding logic"""
        if isinstance(image, str):  # 如果是路径，先加载图片
            from PIL import Image
            image = Image.open(image)

        # 转换为tensor并预处理
        image_array = np.array(image)
        if len(image_array.shape) == 2:  # 灰度图转RGB
            image_array = np.stack([image_array] * 3, axis=-1)

        # 使用现有的图像预处理和VAE编码逻辑
        image_tensor, _, height, width = self.adjust_image_size([image])
        _, image_latents = self.prepare_latents(
            image_tensor, 1, self.num_channels_latents,
            height, width, self.weight_dtype, self.accelerator.device
        )
        return image_latents[0].cpu()  # 返回CPU上的latents以节省显存

    def _encode_prompt_for_validation(self, prompt, control_image=None):
        """Encode prompt for validation using existing Qwen VL encoding logic"""
        # 准备control image用于prompt encoding
        if isinstance(control_image, str):
            from PIL import Image
            control_image = Image.open(control_image)

        control_image_processed = [control_image] if control_image else None

        # 复用现有的encode_prompt方法
        prompt_embeds, prompt_embeds_mask = self.encode_prompt(
            image=control_image_processed,
            prompt=[prompt],
            device=self.accelerator.device,
        )
        return {
            'prompt_embeds': prompt_embeds[0].cpu(),  # 返回CPU上的embeddings
            'prompt_embeds_mask': prompt_embeds_mask[0].cpu()
        }

    def _generate_latents_for_validation(self, cached_sample):
        """Generate latents using cached embeddings and current model"""
        # 使用缓存的embeddings进行推理生成
        text_embeddings = cached_sample['text_embeddings']
        control_latents = cached_sample.get('control_latents')

        # 移动embeddings到GPU
        prompt_embeds = text_embeddings['prompt_embeds'].to(self.accelerator.device)
        prompt_embeds_mask = text_embeddings['prompt_embeds_mask'].to(self.accelerator.device)

        if control_latents is not None:
            control_latents = control_latents.to(self.accelerator.device)

        # 简化的推理逻辑
        batch_size = 1
        height, width = 512, 512  # 固定尺寸用于validation

        # 准备latents
        latents, _ = self.prepare_latents(
            None, batch_size, self.num_channels_latents,
            height, width, self.weight_dtype, self.accelerator.device
        )

        # 使用简化的推理步骤 (fewer steps for validation)
        num_inference_steps = 10
        timesteps = torch.linspace(1000, 0, num_inference_steps, device=self.accelerator.device)

        with torch.no_grad():
            for t in timesteps:
                # 准备模型输入
                if control_latents is not None:
                    packed_input = torch.cat([latents, control_latents], dim=1)
                else:
                    packed_input = latents

                # 准备其他输入
                img_shapes = [[(1, height // self.vae_scale_factor // 2, width // self.vae_scale_factor // 2)] * 2] * batch_size
                txt_seq_lens = prompt_embeds_mask.sum(dim=1).tolist()

                noise_pred = self.transformer(
                    hidden_states=packed_input,
                    timestep=t.expand(latents.shape[0]) / 1000,
                    guidance=None,
                    encoder_hidden_states_mask=prompt_embeds_mask,
                    encoder_hidden_states=prompt_embeds,
                    img_shapes=img_shapes,
                    txt_seq_lens=txt_seq_lens,
                    return_dict=False,
                )[0]

                # 只取前面对应latents的部分
                noise_pred = noise_pred[:, :latents.size(1)]

                # 简化的调度器步骤
                latents = latents - 0.1 * noise_pred  # 简化的更新规则

        return latents

    def _decode_latents_for_validation(self, latents):
        """Decode latents to image using existing VAE decoder"""
        # 复用现有的VAE解码逻辑
        latents = self._unpack_latents(latents, 512, 512, self.vae_scale_factor)

        # 反标准化
        latents_mean = torch.tensor(self.vae.config.latents_mean).view(1, self.vae.config.z_dim, 1, 1, 1).to(latents.device, latents.dtype)
        latents_std = torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(latents.device, latents.dtype)
        latents = latents * latents_std + latents_mean

        # 调整维度格式
        latents = latents.permute(0, 2, 1, 3, 4)

        # 临时将VAE decoder移到GPU
        original_device = self.vae.device
        self.vae.to(self.accelerator.device)

        try:
            with torch.no_grad():
                image = self.vae.decode(latents).sample
                # 后处理
                image = self._postprocess_image(image)
                # 转换为PIL格式
                from PIL import Image as PILImage
                image = PILImage.fromarray(image)
        finally:
            # 恢复VAE设备
            self.vae.to(original_device)

        return image

    def _log_image_to_tensorboard(self, tag, image, global_step):
        """Log image to tensorboard"""
        import numpy as np
        from PIL import Image as PILImage

        # 确保图像格式正确
        if isinstance(image, PILImage.Image):
            image_array = np.array(image)
        else:
            image_array = image

        # 调整尺寸以节省存储
        if max(image_array.shape[:2]) > 512:
            from PIL import Image as PILImage
            pil_image = PILImage.fromarray(image_array)
            pil_image.thumbnail((512, 512), PILImage.Resampling.LANCZOS)
            image_array = np.array(pil_image)

        # 转换为CHW格式用于tensorboard
        if len(image_array.shape) == 3:
            image_tensor = torch.from_numpy(image_array).permute(2, 0, 1).float() / 255.0
        else:
            image_tensor = torch.from_numpy(image_array).float()

        self.accelerator.log({tag: image_tensor}, step=global_step)

    def _log_text_to_tensorboard(self, tag, text, global_step):
        """Log text to tensorboard"""
        self.accelerator.log({tag: text}, step=global_step)

    def fit(self, train_dataloader):
        # ... 现有setup代码 ...
        self.setup_accelerator()
        self.load_model()
        # ... 其他现有代码 ...

        # 添加validation sampling setup (如果启用)
        validation_sampler = None
        if hasattr(self.config.logging, 'sampling') and self.config.logging.sampling.enable:
            from qflux.validation.validation_sampler import ValidationSampler

            validation_sampler = ValidationSampler(
                config=self.config.logging.sampling,
                accelerator=self.accelerator,
                weight_dtype=self.weight_dtype
            )

            # Setup validation dataset
            validation_sampler.setup_validation_dataset(train_dataloader.dataset)

            # Cache embeddings for validation using trainer methods
            embeddings_config = {
                'cache_vae_embeddings': True,       # Cache VAE latents
                'cache_text_embeddings': True,      # Cache Qwen text embeddings
            }
            validation_sampler.cache_embeddings(self, embeddings_config)

        # ... 现有训练循环 ...
        for epoch in range(start_epoch, self.config.train.num_epochs):
            for _, batch in enumerate(train_dataloader):
                # ... 现有training step代码 ...

                if self.accelerator.sync_gradients:
                    # ... 现有logging代码 ...

                    # 添加validation sampling
                    if validation_sampler and validation_sampler.should_run_validation(self.global_step):
                        try:
                            validation_sampler.run_validation_loop(
                                global_step=self.global_step,
                                trainer=self  # 传入trainer实例
                            )
                        except Exception as e:
                            self.accelerator.print(f"Validation sampling failed: {e}")
```

## 简化设计的优势

### 1. **最小侵入性** ✅
- 无需修改trainer的类继承结构
- 无需添加抽象方法
- 只在fit方法中添加少量代码

### 2. **易于实现** ✅
- 不需要复杂的mixin设计
- 直接使用工具类，简单明了
- 每个trainer的集成代码几乎相同

### 3. **向后兼容** ✅
- 现有trainer代码结构完全不变
- 通过配置控制是否启用
- 不影响现有功能

### 4. **性能优化** ✅
- 仍然保持embedding缓存机制
- 智能设备管理
- 错误处理不影响训练

## 文件结构 (按功能划分)

```
src/
├── trainer/                            # 新增trainer目录
│   ├── __init__.py
│   ├── flux_kontext_trainer.py         # 移动到trainer目录
│   └── qwen_image_edit_trainer.py      # 移动到trainer目录
├── validation/
│   ├── __init__.py
│   └── validation_sampler.py           # 单一采样器类
└── data/
    └── config.py                       # 简化配置
```

## Trainer文件重组

将现有的trainer文件移动到专门的`trainer`目录中：

1. **移动文件**:
   - `src/flux_kontext_trainer.py` → `src/trainer/flux_kontext_trainer.py`
   - `src/qwen_image_edit_trainer.py` → `src/trainer/qwen_image_edit_trainer.py`

2. **更新import路径**:
   ```python
   # 在main.py中更新import
   if trainer_type == 'QwenImageEdit':
       from qflux.trainer.qwen_image_edit_trainer import QwenImageEditTrainer as Trainer
   elif trainer_type == 'FluxKontext':
       from qflux.trainer.flux_kontext_trainer import FluxKontextLoraTrainer as Trainer
   ```

3. **新增validation方法**:
   - 每个trainer都需要添加5个validation相关方法：
     - `_encode_vae_image_for_validation()`: 复用现有VAE编码逻辑
     - `_encode_prompt_for_validation()`: 复用现有prompt编码逻辑
     - `_generate_latents_for_validation()`: 使用缓存embeddings生成latents
     - `_decode_latents_for_validation()`: 新增，用于解码latents到图像
     - `_log_image_to_tensorboard()` & `_log_text_to_tensorboard()`: 日志记录方法

## 使用方式

### 1. 文件重组
首先需要重组文件结构：
- 将trainer文件移动到`src/trainer/`目录
- 更新`main.py`中的import路径

### 2. 添加validation方法
每个trainer需要添加5个validation相关方法，这些方法主要复用现有逻辑：
- 编码方法复用现有的`encode_prompt`、VAE编码等
- 解码方法新增`_decode_latents_for_validation`
- 生成方法使用简化的推理逻辑（10步而非完整推理）

### 3. fit方法集成
在fit方法中只需要添加几行代码：
- 检查配置是否启用sampling
- 创建ValidationSampler实例并传入trainer
- 在训练循环中调用validation sampling

## 核心优势

### 1. **复用现有逻辑** ✅
- VAE编码：复用`encode_vae_image`、`preprocess_image`等
- Prompt编码：复用`encode_prompt`方法
- 推理生成：复用`prepare_latents`、`transformer`等
- 只需新增`decode_latents`功能

### 2. **最小代码修改** ✅
- 主要是添加validation方法，现有逻辑不变
- fit方法只添加10-15行代码
- 通过配置控制，不影响现有功能

### 3. **智能资源管理** ✅
- embeddings缓存在CPU，使用时移到GPU
- VAE decoder临时移到GPU进行解码后恢复
- 简化推理（10步）节省计算资源

### 4. **统一接口设计** ✅
- ValidationSampler通过trainer实例调用方法
- 两个不同trainer使用相同的集成模式
- 易于扩展到其他trainer
