# 1.5.2 Hugging Face Dataset Support

## Overview
This document outlines the design for adding Hugging Face dataset support to the existing `ImageDataset` class while maintaining full backward compatibility with local file-based datasets.

## Design Goals
1. Enable loading Hugging Face datasets through the existing `dataset_path` parameter
2. Automatically detect whether the path is a local directory or Hugging Face repository
3. Maintain all existing functionality and preprocessing pipeline
4. Require minimal code changes

## Implementation Approach

### 1. Path Detection Logic
The `ImageDataset` class will detect whether `dataset_path` refers to a local directory or a Hugging Face repository:

```python
def _is_huggingface_repo(self, path: str) -> bool:
    """
    Detect if the path is a Hugging Face repository ID.

    HF repo patterns:
    - username/dataset-name
    - organization/dataset-name
    - dataset-name (for datasets without namespace)

    Local path patterns:
    - /absolute/path/to/dataset
    - ./relative/path
    - ../relative/path
    - path/without/leading/slash (treated as relative)
    """
    # Check if it's an absolute path
    if os.path.isabs(path):
        return False

    # Check if it's a relative path with ./ or ../
    if path.startswith('./') or path.startswith('../'):
        return False

    # Check if the path exists locally
    if os.path.exists(path):
        return False

    # Check HF repo pattern: should contain at most one '/'
    # and not contain invalid characters for repo names
    parts = path.split('/')
    if len(parts) <= 2 and all(part.replace('-', '').replace('_', '').isalnum() for part in parts):
        return True

    return False
```

### 2. Data Loading Modifications

#### 2.1 Modified `__init__` Method
```python
def __init__(self, data_config):
    # ... existing initialization code ...

    # Support for both single and multiple dataset paths
    if isinstance(dataset_path, (list, tuple)):
        self.dataset_paths = list(dataset_path)
    else:
        self.dataset_paths = [dataset_path]

    # Initialize HF dataset storage
    self.hf_datasets = {}

    # Load data from either local or HF sources
    self._load_all_datasets()
```

#### 2.2 New Dataset Loading Method
```python
def _load_all_datasets(self):
    """Load datasets from local directories or Hugging Face repositories."""
    self.all_samples = []  # Unified list of all samples
    self.local_files = []  # For backward compatibility

    for dataset_path in self.dataset_paths:
        if self._is_huggingface_repo(dataset_path):
            # Load from Hugging Face (lazy loading)
            self._load_huggingface_dataset(dataset_path)
        else:
            # Load from local directory (existing logic)
            local_files = self._load_local_dataset(dataset_path)
            # Add to unified sample list
            for file_info in local_files:
                file_info['dataset_type'] = 'local'
                file_info['global_index'] = len(self.all_samples)
                self.all_samples.append(file_info)
            self.local_files.extend(local_files)

    # For backward compatibility
    self.image_files = self.all_samples

def __len__(self):
    """Return total number of samples across all datasets."""
    return len(self.all_samples)
```

#### 2.3 Hugging Face Dataset Loading (Lazy Loading)
```python
def _load_huggingface_dataset(self, repo_id: str):
    """
    Load dataset from Hugging Face using lazy loading approach.

    No enumeration happens here - we just store the dataset object
    and metadata for later access.
    """
    from src.utils.hugginface import load_editing_dataset

    # Load the dataset (this is fast, just creates the dataset object)
    dataset = load_editing_dataset(repo_id, split=self.data_config.get('split', 'train'))

    # Store HF dataset reference
    dataset_info = {
        'type': 'huggingface',
        'repo_id': repo_id,
        'dataset': dataset,
        'length': len(dataset),  # This is typically cached by HF datasets
        'start_idx': len(self.all_samples)  # Track where this dataset starts
    }

    self.hf_datasets[repo_id] = dataset_info

    # Add entries for each sample without iterating
    # We'll create lightweight placeholders
    for idx in range(dataset_info['length']):
        sample_ref = {
            'dataset_type': 'huggingface',
            'repo_id': repo_id,
            'index': idx,
            'global_index': dataset_info['start_idx'] + idx
        }
        self.all_samples.append(sample_ref)
```

### 3. Modified `__getitem__` Method

```python
def __getitem__(self, idx):
    """Get item with support for both local and HF datasets."""
    sample_info = self.all_samples[idx]

    # Handle different dataset types
    if sample_info.get('dataset_type') == 'huggingface':
        # Load from HF dataset (direct access, no enumeration needed)
        dataset_info = self.hf_datasets[sample_info['repo_id']]
        hf_item = dataset_info['dataset'][sample_info['index']]

        # Convert PIL images to numpy arrays
        # Target image
        target_pil = hf_item['target_image']
        image_numpy = np.array(target_pil.convert('RGB'))

        # Control images (support multiple)
        control_images = hf_item['control_images']
        if isinstance(control_images, list) and len(control_images) > 0:
            # Use the first control image for now (can be extended)
            control_pil = control_images[0]
            control_numpy = np.array(control_pil.convert('RGB'))
        else:
            control_numpy = np.array(control_images.convert('RGB'))

        # Mask (optional)
        mask_numpy = None
        if hf_item.get('control_mask') is not None:
            mask_pil = hf_item['control_mask']
            mask_numpy = np.array(mask_pil.convert('L'))  # Grayscale

        # Get prompt
        prompt = hf_item['prompt']

    else:
        # Existing local file loading logic
        with open(sample_info['caption'], 'r', encoding='utf-8') as f:
            prompt = f.read().strip()

        # Load images using cv2 (existing code)
        image_numpy = cv2.imread(sample_info['image'])
        image_numpy = cv2.cvtColor(image_numpy, cv2.COLOR_BGR2RGB)

        control_numpy = cv2.imread(sample_info['control'])
        control_numpy = cv2.cvtColor(control_numpy, cv2.COLOR_BGR2RGB)

        mask_numpy = None
        if sample_info.get('mask_file') and os.path.exists(sample_info['mask_file']):
            mask_numpy = cv2.imread(sample_info['mask_file'], 0)

    # Apply preprocessing (same for both local and HF)
    # ... rest of the existing preprocessing code ...
    # (cropping, resizing, etc.)
```

### 4. Configuration Examples

The configuration remains exactly the same. Users can now pass either local paths or Hugging Face repository IDs:

```yaml
# Example 1: Local dataset (no change)
data:
  class_path: "src.data.dataset.ImageDataset"
  init_args:
    dataset_path: "/raid/lilong/data/face_seg/train/"
    image_size: [512, 512]
    # ... other parameters

# Example 2: Single Hugging Face dataset
data:
  class_path: "src.data.dataset.ImageDataset"
  init_args:
    dataset_path: "username/face-segmentation-dataset"  # HF repo ID
    split: "train"  # Optional, for HF datasets
    image_size: [512, 512]
    # ... other parameters

# Example 3: Multiple datasets (mixed local and HF)
data:
  class_path: "src.data.dataset.ImageDataset"
  init_args:
    dataset_path:
      - "/local/dataset/path"
      - "hf-user/dataset1"
      - "./relative/local/path"
      - "organization/dataset2"
    split: "train"
    image_size: [512, 512]
    # ... other parameters
```

### 5. Key Implementation Details

#### 5.1 Lazy Loading Benefits
- **No enumeration during initialization**: The dataset object is created instantly
- **Fast startup**: Even with millions of samples, initialization is immediate
- **Memory efficient**: Only loads data when actually accessed
- **Scalable**: Works well with both small and large datasets

#### 5.2 Caching Compatibility
For Hugging Face datasets, the cache hash generation will use:
- Repository ID + index for image/control hashes
- Repository ID + index + prompt for prompt hashes

#### 5.3 Error Handling
- Clear error messages if HF dataset doesn't exist
- Fallback to treating as local path if HF loading fails
- Validate HF dataset schema matches expected format

#### 5.4 Performance Optimization
- **Zero-cost initialization**: Only `len(dataset)` is called, which is typically cached
- **On-demand loading**: Data is loaded only when `__getitem__` is called
- **PIL to numpy conversion**: Happens on-demand to minimize memory usage
- **Support for dataset streaming**: For very large datasets (future enhancement)

### 6. Benefits of This Approach

1. **Zero Breaking Changes**: Existing configurations work without any modifications
2. **Intuitive Usage**: Users can simply provide HF repo IDs in `dataset_path`
3. **Minimal Code Changes**: Only modify data loading logic, all preprocessing remains the same
4. **Flexible**: Supports mixing local and HF datasets in the same training run
5. **Consistent Interface**: The rest of the codebase doesn't need to know whether data comes from local files or HF

### 7. Testing Strategy

1. Test path detection logic with various path formats
2. Verify seamless loading of both local and HF datasets
3. Ensure preprocessing pipeline works identically for both sources
4. Test mixed dataset configurations
5. Validate caching works correctly for HF datasets

## Summary

This design seamlessly integrates Hugging Face dataset support into the existing `ImageDataset` class by:
- Adding intelligent path detection to distinguish between local directories and HF repositories
- Using lazy loading to avoid expensive enumeration during initialization
- Converting HF dataset format to the expected internal format on-demand
- Maintaining all existing preprocessing and caching functionality
- Requiring no changes to configuration structure or downstream code

The lazy loading approach ensures that:
- Dataset initialization is instantaneous regardless of dataset size
- Memory usage is optimized by loading data only when needed
- The system scales efficiently from small to very large datasets
