# DreamOmni2: Multimodal Instruction-based Editing and Generation - Reading Notes

## Document Information
- **Paper**: DreamOmni2: Multimodal Instruction-based Editing and Generation
- **ArXiv**: arXiv:2510.06679v1
- **Paper URL**: https://arxiv.org/html/2510.06679v1
- **Code**: https://github.com/dvlab-research/DreamOmni2
- **Created**: 2025-10-30
- **Purpose**: Comprehensive reading notes on DreamOmni2's principles, implementation details, and technical innovations

---

## 1. Core Problem Statement

### 1.1 Limitations of Existing Methods

**Instruction-based Editing** (e.g., Brooks et al., 2023):
- ❌ Relies solely on language instructions
- ❌ Often fails to capture specific editing details
- ❌ Example: "make the bag have the same pattern as the dress" - difficult to describe complex patterns with words alone
- ✅ **Solution**: Need multimodal instructions (text + reference images)

**Subject-driven Generation** (e.g., Xiao et al., 2025):
- ❌ Mainly focuses on concrete objects or people
- ❌ Limited research on abstract attributions (texture, material, posture, hairstyle, design style)
- ✅ **Solution**: Extend to include abstract concepts from multiple reference images

### 1.2 DreamOmni2's Contributions

1. **Novel Tasks**:
   - Multimodal instruction-based editing (text + image instructions)
   - Multimodal instruction-based generation (multiple reference images with abstract/concrete concepts)

2. **Data Pipeline**: Comprehensive 3-step data synthesis pipeline

3. **Architecture Innovations**:
   - Index encoding for multi-image distinction
   - Position encoding shift to prevent pixel confusion
   - Joint VLM training for complex instruction understanding

---

## 2. Data Preparation Pipeline

DreamOmni2 introduces a **3-step data synthesis pipeline** that systematically creates training data for both tasks.

### 2.1 Step 1: Feature Mixing for Image Pair Generation

**Objective**: Create pairs of images sharing the same abstract attribute or concrete object.

**Method**: **Feature Mixing Scheme**

**Process**:
```
1. Generate two batches of images
2. Exchange attention features between batches
3. Generate pairs with shared attributes/objects
4. Split pairs without edge blending artifacts
```

**Key Innovation**: Instead of diptych method (Wu et al., 2025c):
- ✅ **Higher success rate**: More reliable pair generation
- ✅ **Better resolution**: Produces higher quality images
- ✅ **Clean edges**: Completely eliminates content blending at edges when splitting pairs

**Why Important**:
- Provides high-quality training pairs for extraction model
- Eliminates artifacts that would confuse the model
- Enables extraction of both concrete objects and abstract attributes

### 2.2 Step 2: Extraction Model Training & Editing Data Creation

**Part A: Train Extraction Model**

**Process**:
1. Use image pairs from Step 1 (pairs sharing same attribute/object)
2. Train a generation-editing model as an extraction model
3. Model learns to extract concrete objects or abstract attributions from given image
4. Model generates another image based on instructions

**Advantages over Previous Methods** (segmentation/detection-based):
- ✅ **Handles abstract concepts**: Can extract textures, styles, poses, not just objects
- ✅ **Handles occluded objects**: Doesn't require perfect segmentation
- ✅ **More diverse outputs**: Generates varied reference images

**Part B: Generate Multimodal Instruction-based Editing Training Data**

**Data Structure**: Each training sample contains:
- **Target image**: Generated using T2I model based on keywords OR selected from real image database
- **Source image**: Created by transforming target image using instruction-based editing model (Batifol et al., 2025) - transforms content defined by selected keyword
- **Editing instruction**: Text instruction describing the edit
- **Multiple reference images**: Generated by extraction model based on keywords from target image

**Process Flow**:
```
Target Image (T2I or real DB)
    ↓
Extract keyword → Extraction Model → Reference Images
    ↓
Instruction-based Editing Model → Source Image
    ↓
Create Editing Instruction Text
    ↓
Training Sample: (target_image, source_image, instruction, reference_images)
```

### 2.3 Step 3: Multimodal Instruction-based Generation Data Creation

**Objective**: Create data for generating images from multiple reference images.

**Process**:
- Apply extraction model to generate several reference images based on keywords from source images created in Step 2
- Combine multiple reference images with generation instructions

**Result**: Training data with structure:
```
(reference_images: List, generation_instruction: str, target_image: Tensor)
```

---

## 3. Model Architecture Innovations

### 3.1 Multi-Image Input Handling

**Challenge**: Current SOTA models (e.g., Batifol et al., 2025) cannot handle multiple image inputs simultaneously.

**Solution**: Two key innovations:

#### 3.1.1 Index Encoding

**Purpose**: Help model identify which input image is being referenced in instructions.

**Implementation**:
```python
# From pipeline_dreamomni2.py:697
for i, image in enumerate(images):
    image_ids = self._prepare_latent_image_ids(...)
    image_ids[..., 0] = i + 1  # Index encoding: 1, 2, 3, ...
```

**Image ID Structure**: `[domain, row, col]`
- Domain ID (index 0): `1` for first image, `2` for second, `3` for third, etc.
- Row (index 1): Row position within image
- Col (index 2): Column position within image

**Why Critical**:
- Enables instructions like "make the bag have the same pattern as the dress in image 2"
- Model can correctly reference specific images in multi-image scenarios
- Allows distinction between multiple input images

#### 3.1.2 Position Encoding Shift Scheme

**Purpose**: Prevent pixel confusion and copy-paste effects when concatenating multiple images.

**Implementation**:
```python
# From pipeline_dreamomni2.py:667-702
h_offset = 0
w_offset = 0
tp_image_latents = []
tp_image_ids = []

for i, image in enumerate(images):
    # ... encode and pack image ...
    image_ids[..., 0] = i + 1  # Index encoding
    image_ids[..., 2] += w_offset  # ⭐ KEY: Cumulative column offset

    tp_image_latents.append(image_latents)
    tp_image_ids.append(image_ids)

    # Accumulate offsets for next image
    h_offset += image_latent_height // 2
    w_offset += image_latent_width // 2

# Concatenate along sequence dimension
image_latents = torch.cat(tp_image_latents, dim=1)  # [B, sum(seq_i), C]
image_ids = torch.cat(tp_image_ids, dim=0)  # [sum(seq_i), 3]
```

**Why Critical - RoPE Understanding**:

Flux Transformer uses **3D RoPE (Rotary Position Embedding)** based on `[domain, row, col]` coordinates:

**Without Offsets** (❌ Problem):
```
Image 1: domain=1, col=[0, 1, 2, ..., W1-1]
Image 2: domain=2, col=[0, 1, 2, ..., W2-1]  ⚠️ Both start from 0!
```
- RoPE thinks both images overlap spatially (same column positions)
- Results in pixel confusion and copy-paste artifacts

**With Offsets** (✅ Solution):
```
Image 1: domain=1, col=[0, 1, 2, ..., W1-1]
Image 2: domain=2, col=[W1, W1+1, W1+2, ..., W1+W2-1]  ✅ Continuous!
```
- RoPE correctly understands spatial relationships
- No pixel confusion
- No copy-paste artifacts

**Offset Calculation**:
- `w_offset` accumulates: `w_offset += image_latent_width // 2`
- `h_offset` accumulates: `h_offset += image_latent_height // 2`
- Why `// 2`? Because of `_pack_latents` which packs spatial dimensions by factor of 2

#### 3.1.3 Latent Packing and Concatenation

**Packing Process** (`_pack_latents`):
```python
# From pipeline_dreamomni2.py:572-578
def _pack_latents(latents, batch_size, num_channels_latents, height, width):
    # Reshape: [B, C, H, W] → [B, H//2, W//2, C*4]
    latents = latents.view(batch_size, num_channels_latents, height // 2, 2, width // 2, 2)
    latents = latents.permute(0, 2, 4, 1, 3, 5)
    latents = latents.reshape(batch_size, (height // 2) * (width // 2), num_channels_latents * 4)
    return latents
```

**Effect**:
- Spatial dimensions reduced by factor of 2 in each direction
- That's why offset accumulation uses `// 2`

**Concatenation**:
```python
# Multiple images concatenated along sequence dimension
image_latents = torch.cat(tp_image_latents, dim=1)  # [B, sum(seq_i), C]
image_ids = torch.cat(tp_image_ids, dim=0)  # [sum(seq_i), 3]
```

**Final Structure**:
- **Text tokens**: `[B, seq_text, C]` with `text_ids: [seq_text, 3]` (domain=0)
- **Image latents**: `[B, sum(seq_i), C]` with `image_ids: [sum(seq_i), 3]` (domain=1,2,3,...)
- **Target latents**: `[B, seq_target, C]` with `latent_ids: [seq_target, 3]` (domain=0)
- **Combined**: `torch.cat([latent_ids, image_ids], dim=0)` → `[seq_total, 3]`

### 3.2 Joint Training with VLM

**Problem**:
- Generation/editing model instructions are typically simple
- Real-world user instructions are often irregular and logically complex
- Model needs to understand complex user intentions

**Solution**: Joint Training Scheme

**Approach**:
- Jointly train generation/editing model with VLM (Vision-Language Model)
- VLM pre-trained on large-scale corpora can better understand complex intentions
- VLM translates complex instructions into a form the generation/editing model can process

**Benefits**:
- Better understanding of complex, irregular instructions
- Improved instruction interpretation
- More robust handling of real-world user queries

**Note**: Implementation details not fully described in paper, but conceptually:
- VLM processes instructions → generates intermediate representations
- Generation/editing model uses these representations + image inputs → generates output

---

## 4. Implementation Details from Code

### 4.1 Pipeline Interface

**Input Format**:
```python
def __call__(
    self,
    images: Optional[List[PipelineImageInput]] = None,  # ⭐ List of images
    prompt: Union[str, List[str]] = None,
    ...
):
```

**Key Difference from Flux Kontext**:
```python
# Flux Kontext: Single image
image: Optional[PipelineImageInput] = None

# DreamOmni2: Multiple images
images: Optional[List[PipelineImageInput]] = None
```

### 4.2 Image Preprocessing

**From `__call__` method** (lines 966-982):
```python
if images is not None:
    tp_images = []
    for img in images:
        image_height, image_width = self.image_processor.get_default_height_width(img)
        aspect_ratio = image_width / image_height

        if _auto_resize:
            # Use preferred Kontext resolutions
            _, image_width, image_height = min(
                (abs(aspect_ratio - w / h), w, h)
                for w, h in PREFERRED_KONTEXT_RESOLUTIONS
            )

        image_width = image_width // multiple_of * multiple_of
        image_height = image_height // multiple_of * multiple_of

        image = self.image_processor.resize(image, image_height, image_width)
        image = self.image_processor.preprocess(image, image_height, image_width)
        tp_images.append(image)

    images = tp_images
```

**Key Points**:
- Each image can have different resolution
- Resized to preferred Kontext resolutions if `_auto_resize=True`
- Dimensions made divisible by `multiple_of = vae_scale_factor * 2`

### 4.3 Latent Preparation Flow

**Complete Flow**:
```python
def prepare_latents(self, images, batch_size, ...):
    h_offset = 0
    w_offset = 0
    tp_image_latents = []
    tp_image_ids = []

    for i, image in enumerate(images):
        # 1. Encode with VAE
        image_latents = self._encode_vae_image(image=image, generator=generator)

        # 2. Handle batch expansion
        if batch_size > image_latents.shape[0]:
            additional_image_per_prompt = batch_size // image_latents.shape[0]
            image_latents = torch.cat([image_latents] * additional_image_per_prompt, dim=0)

        # 3. Pack latents: [B, C, H, W] → [B, H//2*W//2, C*4]
        image_latent_height, image_latent_width = image_latents.shape[2:]
        image_latents = self._pack_latents(
            image_latents, batch_size, num_channels_latents,
            image_latent_height, image_latent_width
        )

        # 4. Prepare image IDs: [H//2*W//2, 3]
        image_ids = self._prepare_latent_image_ids(
            batch_size, image_latent_height // 2, image_latent_width // 2,
            device, dtype
        )

        # 5. Apply index encoding and position shift
        image_ids[..., 0] = i + 1  # Domain ID: 1, 2, 3, ...
        image_ids[..., 2] += w_offset  # Cumulative column offset

        # 6. Store and accumulate
        tp_image_latents.append(image_latents)
        tp_image_ids.append(image_ids)
        h_offset += image_latent_height // 2
        w_offset += image_latent_width // 2

    # 7. Concatenate all images
    image_latents = torch.cat(tp_image_latents, dim=1)  # [B, sum(seq_i), C]
    image_ids = torch.cat(tp_image_ids, dim=0)  # [sum(seq_i), 3]

    return latents, image_latents, latent_ids, image_ids
```

### 4.4 ID Combination

**From `__call__` method** (line 998):
```python
if image_ids is not None:
    latent_ids = torch.cat([latent_ids, image_ids], dim=0)  # dim 0 is sequence dimension
```

**Final Structure**:
- `latent_ids`: IDs for target latents (domain=0)
- `image_ids`: IDs for reference images (domain=1,2,3,...)
- Combined: `[seq_total, 3]` where `seq_total = seq_target + sum(seq_i)`

---

## 5. Comparison with Flux Kontext

| Aspect | Flux Kontext | DreamOmni2 |
|--------|--------------|------------|
| **Input Format** | Single `image` | Multiple `images: List` |
| **Processing Level** | Trainer-level multi-image | Pipeline-level multi-image |
| **Image IDs** | Fixed `domain=1` | Dynamic `domain=i+1` |
| **Position Encoding** | No offsets | Cumulative offsets (`h_offset`, `w_offset`) |
| **Latent Concatenation** | Trainer concatenates separately | Pipeline concatenates with offsets |
| **Use Case** | Single reference image | Multiple reference images with abstract/concrete concepts |

**Key Insight**: If Flux Kontext Trainer adds cumulative offsets, it would be algorithmically equivalent to DreamOmni2 Pipeline!

---

## 6. Training Framework

### 6.1 Data Flow Summary

```
Step 1: Feature Mixing
  → Image pairs (same attribute/object)
    ↓
Step 2: Extraction Model Training
  → Trained extraction model
  → Multimodal editing data (target, source, instruction, references)
    ↓
Step 3: Generation Data Creation
  → Multimodal generation data (references, instruction, target)
```

### 6.2 Training Data Structure

**Editing Task**:
```python
{
    "target_image": Tensor,        # [B, C, H, W]
    "source_image": Tensor,         # [B, C, H, W]
    "images": List[Tensor],         # Multiple reference images
    "prompt": str,                  # Editing instruction
    "control_latents": Tensor,      # Concatenated [B, sum(seq_i), C]
    "control_ids": Tensor,         # Concatenated [sum(seq_i), 3] with offsets
}
```

**Generation Task**:
```python
{
    "images": List[Tensor],         # Multiple reference images
    "prompt": str,                  # Generation instruction
    "target_image": Tensor,        # [B, C, H, W]
    "control_latents": Tensor,     # Concatenated [B, sum(seq_i), C]
    "control_ids": Tensor,         # Concatenated [sum(seq_i), 3] with offsets
}
```

---

## 7. Key Technical Insights

### 7.1 Why Cumulative Offsets Matter

**Problem**: When multiple images are concatenated, their position encodings overlap if they all start from (0,0).

**Solution**: Cumulative offsets ensure spatial continuity:
- Image 1: `col=[0..W1-1]`
- Image 2: `col=[W1..W1+W2-1]` ← Continuous!

**RoPE Impact**:
- RoPE calculates attention based on relative positions
- Without offsets: Model thinks images overlap → pixel confusion
- With offsets: Model understands spatial continuity → correct generation

### 7.2 Index Encoding vs Position Encoding

**Index Encoding** (`image_ids[..., 0] = i + 1`):
- Purpose: Distinguish which image is which
- Used by: Model to reference specific images in instructions
- Example: "Apply pattern from image 2"

**Position Encoding** (`image_ids[..., 1:3]` with offsets):
- Purpose: Spatial understanding for RoPE
- Used by: Transformer attention mechanism
- Ensures: Correct spatial relationships when concatenated

**Both are necessary**:
- Index encoding → "which image"
- Position encoding → "where in space"

### 7.3 Packing and Offset Relationship

**Why `// 2` in offset accumulation?**
- `_pack_latents` reduces spatial dimensions by factor of 2
- Original: `[B, C, H, W]`
- Packed: `[B, (H//2)*(W//2), C*4]`
- Image IDs created for packed dimensions: `[H//2, W//2]`
- So offset must also use packed dimensions: `w_offset += image_latent_width // 2`

---

## 8. Benchmark

DreamOmni2 proposes comprehensive benchmarks:
- **205 multimodal instruction-based editing test cases**
- **114 instruction-based generation test cases**
- Input reference images range from 1 to 5
- Covers diverse local/global attributes and concrete objects
- DreamOmni2 Benchmark will be released

---

## 9. Practical Implementation Notes

### 9.1 For Trainer Development

**Key Requirements**:
1. **Multi-image Support**: Process multiple images in single forward pass
2. **Index Encoding**: Assign domain IDs (`1, 2, 3, ...`) to each image
3. **Cumulative Offsets**: Calculate and apply `w_offset` and `h_offset`
4. **Latent Concatenation**: Concatenate along sequence dimension with correct offsets

**Critical Code Pattern**:
```python
h_offset = 0
w_offset = 0
for i, image in enumerate(images):
    # Process image...
    image_ids[..., 0] = i + 1  # Index encoding
    image_ids[..., 2] += w_offset  # Position shift ⭐ KEY
    w_offset += image_latent_width // 2  # Accumulate
    h_offset += image_latent_height // 2
```

### 9.2 Common Pitfalls

**❌ Wrong**: Each image starts from `col=0`
```python
for i, image in enumerate(images):
    image_ids[..., 0] = i + 1
    # Missing: image_ids[..., 2] += w_offset
    # Missing: w_offset accumulation
```

**✅ Correct**: Cumulative offsets
```python
for i, image in enumerate(images):
    image_ids[..., 0] = i + 1
    image_ids[..., 2] += w_offset  # Apply offset
    w_offset += image_latent_width // 2  # Accumulate
```

---

## 10. References

### Papers
- **DreamOmni2**: Bin Xia et al. "DreamOmni2: Multimodal Instruction-based Editing and Generation" arXiv:2510.06679v1
- **Flux Kontext**: FLUX.1-Kontext-dev model
- **Instruction-based Editing**: Brooks et al. (2023), Batifol et al. (2025)
- **Subject-driven Generation**: Xiao et al. (2025)

### Code
- **DreamOmni2 Pipeline**: https://github.com/dvlab-research/DreamOmni2
- **Implementation Reference**: `src/qflux/models/pipeline_dreamomni2.py`

### Documentation
- **Project Plan**: `docs/plan/dreamomni2_trainer_plan.md`
- **Offset Analysis**: `docs/plan/dreamomni2_offset_analysis.md`

---

## 11. Summary

DreamOmni2's core innovations:

1. **Data Pipeline**:
   - Feature mixing for pair generation
   - Extraction model for abstract/concrete concepts
   - Multi-step data synthesis

2. **Architecture**:
   - Index encoding for multi-image distinction
   - Position encoding shift (cumulative offsets) for spatial understanding
   - Joint VLM training for complex instructions

3. **Implementation**:
   - Pipeline-level multi-image handling
   - Cumulative offset calculation (`w_offset`, `h_offset`)
   - Proper latent packing and concatenation

**Key Takeaway**: The cumulative offset mechanism is the critical innovation that enables correct spatial understanding when multiple images are concatenated, preventing pixel confusion and copy-paste artifacts.

---

**End of Reading Notes**
