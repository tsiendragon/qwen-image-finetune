<!-- dec23b68-366b-4ffe-a19f-3400695dabd5 / ea430290-a4f1-48cd-9cea-59bd5f31cb4d -->
# Multi-resolution Training with Per-Sample IDs
Target:
1. æ”¯æŒå¤šç§åˆ†è¾¨ç‡çš„è®­ç»ƒ
2. å•ä¸ªæ ·æœ¬çš„inferenceèµ°ä¹‹å‰çš„é€»è¾‘ï¼Œä¹Ÿå°±æ˜¯å’Œä¹‹å‰çš„é¢„æµ‹ç»“æœåº”è¯¥ä¿æŒä¸€è‡´
3. ä½¿ç”¨ padding å’Œattention mask çš„æ€è·¯å»å®ç°è¿™ä¸ªæƒ³æ³•
4. æœ€å°ä»£ç æ”¹åŠ¨ï¼Œä¸ç ´ååŸæœ¬çš„é€»è¾‘ï¼Œå¯ä»¥æ–°å¢ä¸åŒçš„ branch
5. æ‹†åˆ†ä¸åŒ trainer å…±ç”¨çš„ä»£ç å’Œæ¯ä¸ª trainer éœ€è¦ç‰¹å®šä¿®æ”¹çš„å†…å®¹
## 0. æ¦‚è¿°ä¸å½“å‰è¿›åº¦

ç›®æ ‡ï¼šå…è®¸åŒä¸€ batch å†…çš„æ ·æœ¬ä½¿ç”¨ä¸åŒçš„ç›®æ ‡åˆ†è¾¨ç‡ï¼Œç¡®ä¿æ•°æ®æµï¼ˆé…ç½® â†’ é¢„å¤„ç† â†’ ç¼“å­˜ â†’ collate â†’ trainer â†’ æ¨¡å‹ â†’ lossï¼‰å…¨é“¾è·¯æ­£ç¡®å¤„ç† paddingï¼Œå¹¶ä¿è¯ pad token çš„ attention/output è¢«å±è”½ã€‚

å½“å‰å®Œæˆæƒ…å†µï¼š

- âœ… æ•°æ®ç®¡çº¿ï¼ˆé…ç½®ã€é¢„å¤„ç†ã€collateï¼‰è§„åˆ’å·²æ˜ç¡®ï¼›
- âœ… `FluxTransformer2DModel` å·²æ”¯æŒ `attention_mask`ï¼ˆé›¶å‡º padding tokenï¼Œç”Ÿæˆ additive maskï¼‰ï¼›
- âœ… `tests/models/test_flux_transformer_padding.py` è¦†ç›–ç­‰ä»·æ€§æµ‹è¯•ï¼›
- â³ `FluxTransformer2DModel` ä»æœªå®ç° **per-sample RoPE**ï¼ˆä»å¤ç”¨ diffusers çš„ shared `img_ids` é€»è¾‘ï¼‰ï¼›
- â³ Trainer ç«¯ per-sample latents/idsã€Qwen Trainer è¿ç§»å‡å¾…å®Œæˆã€‚

ä»¥ä¸‹å†…å®¹æ¢å¤åŸæœ‰çš„å®Œæ•´è®¡åˆ’ï¼ŒæŒ‰ç»„ä»¶è¯´æ˜æ‰€æœ‰æ”¹åŠ¨åŠ TODOã€‚é‡ç‚¹æ›´æ–°äº†ã€Œæ¨¡å‹å±‚ / Fluxã€ç« èŠ‚ï¼Œæ ‡è®°ç°çŠ¶ä¸ä¸‹ä¸€æ­¥ã€‚

### 0.1 æ•°æ®æ ¼å¼è§„èŒƒï¼ˆType Definitionsï¼‰

ç»Ÿä¸€çš„ç±»å‹å®šä¹‰å’Œæ•°æ®çº¦å®šï¼š

```python
from typing import List, Tuple
import torch

# æ•°æ®æ ¼å¼è§„èŒƒ
ImgShapes = List[Tuple[int, int, int]]  # [(C, H', W'), ...] æ¯ä¸ªæ ·æœ¬çš„ latent shape
# ç¤ºä¾‹: [(1, 52, 104), (1, 64, 64)] è¡¨ç¤ºä¸¤ä¸ªæ ·æœ¬ï¼Œä¸€ä¸ª 52x104ï¼Œä¸€ä¸ª 64x64

ImgIdsBatched = torch.Tensor  # shape: (B, seq, 3) for per-sample mode
ImgIdsShared = torch.Tensor   # shape: (seq, 3) for shared resolution mode
# img_ids[..., 0] = batch_idx (åœ¨ shared mode æ—¶ä¸º 0)
# img_ids[..., 1] = h_idx (height position index)
# img_ids[..., 2] = w_idx (width position index)

AttentionMask = torch.Tensor  # shape: (B, seq_txt + seq_img), bool or float
# True/1 è¡¨ç¤ºæœ‰æ•ˆ tokenï¼ŒFalse/0 è¡¨ç¤º padding token
```

### 0.2 æ•°æ®æµçº¦å®š

| é˜¶æ®µ | img_shapes æ ¼å¼ | img_ids æ ¼å¼ | è¯´æ˜ |
|------|----------------|-------------|------|
| **é¢„å¤„ç†è¾“å‡º** | N/A | N/A | è¿”å› `height`, `width` æ ‡é‡ |
| **Dataset.__getitem__** | N/A | N/A | å•æ ·æœ¬ï¼Œè¿”å›æ ‡é‡ç»´åº¦ |
| **ç¼“å­˜æ–‡ä»¶** | `torch.tensor([(1, H', W'), ...])` | N/A | ä¿å­˜ä¸º tensorï¼Œæ—  batch ç»´åº¦ |
| **Collate è¾“å‡º** | `List[List[(1, H', W'), ...]]` | N/A | å¤–å±‚ list é•¿åº¦ä¸º batch_size |
| **Trainer prepare_embeddings** | `List[List[(1, H', W'), ...]]` | æ ¹æ®æ¨¡å¼ç”Ÿæˆ | åˆ¤æ–­æ˜¯å¦å¤šåˆ†è¾¨ç‡ |
| **æ¨¡å‹ forward (shared)** | N/A | `(seq, 3)` | å•åˆ†è¾¨ç‡æ¨¡å¼ |
| **æ¨¡å‹ forward (per-sample)** | N/A | `(B, seq, 3)` | å¤šåˆ†è¾¨ç‡æ¨¡å¼ |

---

## 1. é…ç½®å±‚ (`src/data/config.py`)

### 1.1 `ImageProcessorInitArgs`

- æ–°å¢å­—æ®µï¼š
  ```python
  multi_resolutions: Optional[Union[List[int], List[str]]] = None
  ```
- Validatorï¼šè§£æ `"512*768"` ç­‰å­—ç¬¦ä¸²ä¸ºæ•´æ•°é¢ç§¯ï¼›ä¿ç•™æ—§å­—æ®µå…¼å®¹ï¼›
- **å¤šåˆ†è¾¨ç‡åˆ¤æ–­**ï¼šåªè¦ `multi_resolutions` å­˜åœ¨ä¸”éç©ºï¼Œå³å¯ç”¨å¤šåˆ†è¾¨ç‡æ¨¡å¼ï¼Œæ— éœ€é¢å¤–å¼€å…³ï¼›
- ä¼˜å…ˆçº§ï¼š
  1. `target_size`ï¼ˆå›ºå®šå°ºå¯¸ï¼Œç¦ç”¨å¤šåˆ†è¾¨ç‡ï¼‰
  2. `multi_resolutions`ï¼ˆå¤šåˆ†è¾¨ç‡æ¨¡å¼ï¼‰
  3. `target_pixels`ï¼ˆå•ä¸€é¢ç§¯ï¼Œç¦ç”¨å¤šåˆ†è¾¨ç‡ï¼‰

### 1.2 é…ç½®ç¤ºä¾‹

#### 1.2.1 å¤šåˆ†è¾¨ç‡é…ç½®

```yaml
data:
  init_args:
    processor:
      class_path: src.data.preprocess.ImageProcessor
      init_args:
        process_type: fixed_pixels
        # å­˜åœ¨ multi_resolutions å³è‡ªåŠ¨å¯ç”¨å¤šåˆ†è¾¨ç‡
        multi_resolutions:
          - "512*512"    # 262144
          - "640*640"    # 409600
          - "768*512"    # 393216
          - "832*576"    # 479232
        max_aspect_ratio: 3.0  # å¯é€‰ï¼šæœ€å¤§å®½é«˜æ¯”é™åˆ¶

    # Collate é…ç½®
    batch_size: 4
    # æ³¨æ„ï¼šå¤šåˆ†è¾¨ç‡ä¸‹ï¼Œå®é™… batch token æ•°ä¼šå¢åŠ ï¼ˆpaddingï¼‰
    # å»ºè®®æ ¹æ® max_resolution è°ƒæ•´ batch_size

loss:
  use_mask_loss: true
  mask_loss_fn:
    foreground_weight: 2.0
    background_weight: 1.0
  normalize_by_valid_tokens: true  # æ–°å¢ï¼šæŒ‰å®é™… token æ•°å½’ä¸€åŒ–
```

#### 1.2.2 å•åˆ†è¾¨ç‡é…ç½®ï¼ˆå‘åå…¼å®¹ï¼‰

```yaml
data:
  init_args:
    processor:
      init_args:
        # æ–¹å¼ 1: ä½¿ç”¨å›ºå®šå°ºå¯¸
        target_size: [512, 512]

        # æ–¹å¼ 2: ä½¿ç”¨å•ä¸€é¢ç§¯ï¼ˆä¸æä¾› multi_resolutionsï¼‰
        # target_pixels: 262144  # 512*512
```

---

## 2. æ•°æ®é¢„å¤„ç† (`src/data/preprocess.py`)

### 2.1 é€‰æ‹©å€™é€‰é¢ç§¯

- åœ¨ `ImageProcessor.__init__` ä¸­ä¿å­˜ `self.multi_resolutions`ï¼›
- æ–°å¢ `_select_pixels_candidate(orig_w, orig_h)`ï¼š
  - `orig_area = orig_w * orig_h`ï¼›
  - å¯¹æ¯ä¸ªå€™é€‰ `A_i` è®¡ç®— `err = abs(A_i - orig_area) / orig_area`ï¼›
  - è¯¯å·®ç›¸åŒæ—¶é€‰é¢ç§¯æ›´å°è€…ï¼›
  - å¦‚ä»å¹¶åˆ—ï¼Œæ¯”è¾ƒ `calculate_best_resolution` å¾—åˆ°çš„å°ºå¯¸ä¸åŸæ¯”ä¾‹çš„å·®å¼‚ã€‚

### 2.2 `_process_image`

```python
if self.multi_resolutions:
    best_pixels = self._select_pixels_candidate(w, h)
    new_w, new_h = calculate_best_resolution(w, h, best_pixels)
elif self.target_size:
    ...
```

- target imageã€maskã€control å…±ç”¨åŒä¸€ `(new_w, new_h)`ï¼›
- å¤šæ§åˆ¶åˆ†æ”¯é€ä¸ªè°ƒç”¨ï¼›
- `preprocess` è¿”å› `height`, `width`, `height_control`, `width_control`, `height_control_i`, `width_control_i` ä¾› Trainer ä½¿ç”¨ï¼›
- mask è¾“å‡ºä¿æŒ `[H, W]`ï¼Œå€¼åŸŸ `[0, 1]`ã€‚

---

## 3. æ•°æ®é›†ä¸ç¼“å­˜ (`src/data/dataset.py`)

### 3.1 ç¼“å­˜ç­–ç•¥ä¸å…¼å®¹æ€§

#### 3.1.1 ç¼“å­˜ç‰ˆæœ¬ç®¡ç†

åœ¨ `cache_step` ä¿å­˜æ—¶æ·»åŠ ç‰ˆæœ¬æ ‡è®°ï¼š

```python
cache_dict = {
    "version": "2.0",  # æ–°å¢ï¼šæ ‡è®°æ”¯æŒå¤šåˆ†è¾¨ç‡
    "prompt_embeds": ...,
    "pooled_prompt_embeds": ...,
    "img_shapes": torch.tensor(img_shapes_list),  # [(1, H', W'), ...]
    # æ—§ç‰ˆæœ¬ï¼ˆv1.0ï¼‰æ²¡æœ‰ version å’Œ img_shapes å­—æ®µ
}
```

éœ€ä¿å­˜çš„å­—æ®µï¼š
  - `img_shapes`: `torch.tensor([(1, H', W'), ...])`ï¼›
  - æ§åˆ¶åˆ†æ”¯çš„å½¢çŠ¶ `control_shapes`, `control_i_shapes`ï¼›
  - è‹¥å¯ç”¨ mask lossï¼Œä¿å­˜ latent å°ºå¯¸çš„ maskã€‚

#### 3.1.2 åŠ è½½æ—¶çš„å…¼å®¹å¤„ç†

åœ¨ `prepare_cached_embeddings` ä¸­ï¼š

```python
def prepare_cached_embeddings(self, batch: dict):
    cache_version = batch.get("version", "1.0")

    if cache_version == "1.0":
        # æ—§ç¼“å­˜æ ¼å¼ï¼šæ²¡æœ‰ img_shapesï¼Œä» height/width é‡å»º
        batch_size = batch["prompt_embeds"].shape[0]
        height = batch.get("height", self.config.data.default_height)
        width = batch.get("width", self.config.data.default_width)

        # ç”Ÿæˆç»Ÿä¸€çš„ img_shapes
        latent_h = height // self.vae_scale_factor // 2
        latent_w = width // self.vae_scale_factor // 2
        img_shapes = [[(1, latent_h, latent_w)]] * batch_size
        batch["image_latents_shapes"] = img_shapes

        logging.info(f"Loaded v1.0 cache, reconstructed shapes: {img_shapes[0]}")

    elif cache_version == "2.0":
        # æ–°ç¼“å­˜æ ¼å¼ï¼šç›´æ¥ä½¿ç”¨ img_shapes
        img_shapes_tensor = batch["img_shapes"]  # [N, 3]
        batch_size = batch["prompt_embeds"].shape[0]
        img_shapes = img_shapes_tensor.tolist()
        batch["image_latents_shapes"] = [img_shapes] * batch_size

    else:
        raise ValueError(f"Unsupported cache version: {cache_version}")

    # mask å¤„ç†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if "mask" in batch:
        if batch["mask"].dim() == 2:  # [H, W] - æ—§æ ¼å¼
            # éœ€è¦ latent åŒ–
            mask_bhw = batch["mask"].unsqueeze(0)  # [1, H, W]
            mask_latent = map_mask_to_latent(mask_bhw).squeeze(0)  # [seq]
            batch["mask"] = mask_latent
        # else: å·²æ˜¯ [seq] æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
```

#### 3.1.3 ç¼“å­˜è¿ç§»å·¥å…·ï¼ˆå¯é€‰ï¼‰

æä¾›è„šæœ¬ `scripts/migrate_cache_v1_to_v2.py`ï¼š

```python
def migrate_cache(old_path: str, new_path: str):
    """è¿ç§» v1.0 ç¼“å­˜åˆ° v2.0 æ ¼å¼"""
    data = torch.load(old_path)

    # æ·»åŠ  version å­—æ®µ
    data["version"] = "2.0"

    # é‡å»º img_shapesï¼ˆå¦‚æœç¼ºå¤±ï¼‰
    if "img_shapes" not in data:
        height = data.get("height", 512)
        width = data.get("width", 512)
        # å‡è®¾ vae_scale_factor=8, packing=2
        latent_h = height // 8 // 2
        latent_w = width // 8 // 2
        data["img_shapes"] = torch.tensor([(1, latent_h, latent_w)])

    torch.save(data, new_path)
    logging.info(f"Migrated {old_path} -> {new_path}")
```

### 3.2 Collate Function

```python
from src.loss.edit_mask_loss import map_mask_to_latent

def collate_fn(batch):
    ...
    if key == "mask":
        latent_masks = []
        for mask in batch_dict[key]:
            if mask.dim() == 2:
                latent_mask = map_mask_to_latent(mask.unsqueeze(0)).squeeze(0)
            else:
                latent_mask = mask  # ç¼“å­˜æ¨¡å¼
            latent_masks.append(latent_mask)
        batch_dict[key] = pad_to_max_shape(latent_masks)
        continue
    ...
```

- è®°å½• `image_latents_shapes` / `control_latents_shapes`ï¼›
- åµŒå¥— dict ç»§ç»­é€’å½’å¤„ç†ã€‚

---

## 4. Trainer å±‚ï¼ˆä»¥ `FluxKontextLoraTrainer` ä¸ºä¾‹ï¼‰

### 4.1 `prepare_embeddings` ä¸è¾¹ç•Œæƒ…å†µå¤„ç†

#### 4.1.1 `_should_use_multi_resolution_mode` å®Œæ•´å®ç°

```python
def _should_use_multi_resolution_mode(self, batch: dict) -> bool:
    """åˆ¤æ–­æ˜¯å¦éœ€è¦å¯ç”¨å¤šåˆ†è¾¨ç‡æ¨¡å¼

    è¿”å› False çš„æƒ…å†µï¼š
    1. batch_size == 1ï¼ˆå•æ ·æœ¬ç›´æ¥èµ°åŸé€»è¾‘ï¼‰
    2. æ‰€æœ‰æ ·æœ¬å°ºå¯¸å®Œå…¨ä¸€è‡´ï¼ˆä¸éœ€è¦ paddingï¼‰
    3. æœªé…ç½® multi_resolutionsï¼ˆå•åˆ†è¾¨ç‡é…ç½®ï¼‰

    è¿”å› True çš„æƒ…å†µï¼š
    1. é…ç½®äº† multi_resolutions
    2. batch_size > 1
    3. batch ä¸­å­˜åœ¨ä¸åŒå°ºå¯¸çš„æ ·æœ¬
    """
    batch_size = batch["prompt_embeds"].shape[0]
    if batch_size == 1:
        return False

    # æ£€æŸ¥æ˜¯å¦æœ‰ _shapes å­—æ®µï¼ˆcollate è¾“å‡ºï¼‰
    if "image_latents_shapes" in batch:
        shapes = batch["image_latents_shapes"]
        if len(shapes) != batch_size:
            raise ValueError(
                f"image_latents_shapes length {len(shapes)} != batch_size {batch_size}"
            )
        # æ£€æŸ¥æ‰€æœ‰æ ·æœ¬æ˜¯å¦åŒå°ºå¯¸
        first_shape = shapes[0]
        if all(s == first_shape for s in shapes):
            logging.debug("All samples have identical resolution, using shared mode")
            return False
        return True

    # å›é€€ï¼šæ£€æŸ¥ height/width å­—æ®µ
    if "height" in batch and "width" in batch:
        heights = batch["height"] if isinstance(batch["height"], list) else [batch["height"]] * batch_size
        widths = batch["width"] if isinstance(batch["width"], list) else [batch["width"]] * batch_size

        if len(set(zip(heights, widths))) == 1:
            return False  # æ‰€æœ‰æ ·æœ¬åŒå°ºå¯¸
        return True

    # æ— æ³•åˆ¤æ–­ï¼Œé»˜è®¤ Falseï¼ˆä¿å®ˆç­–ç•¥ï¼‰
    logging.warning("Cannot determine resolution mode, falling back to shared mode")
    return False
```

**å…³é”®ç®€åŒ–**ï¼š
- æ˜¯å¦å¤šåˆ†è¾¨ç‡ç”±é…ç½® `multi_resolutions` å†³å®šï¼ˆåœ¨é¢„å¤„ç†é˜¶æ®µå·²ç¡®å®šï¼‰
- è¿è¡Œæ—¶ä»…æ£€æŸ¥ batch å†…æ ·æœ¬å°ºå¯¸æ˜¯å¦ä¸€è‡´

#### 4.1.2 åŒå°ºå¯¸ä¼˜åŒ–

å½“æ£€æµ‹åˆ°æ‰€æœ‰æ ·æœ¬å°ºå¯¸ä¸€è‡´æ—¶ï¼š
- ä¸æ‰§è¡Œ padding
- ä½¿ç”¨åŸ batched `prepare_latents`
- ä¸ä¼ é€’ `attention_mask`
- ç”Ÿæˆ shared `img_ids` (seq, 3)

#### 4.1.3 `prepare_embeddings` æµç¨‹

1. è°ƒç”¨ `_should_use_multi_resolution_mode(batch)` åˆ¤æ–­æ¨¡å¼ï¼›
2. è‹¥å¤šåˆ†è¾¨ç‡ï¼š
   - å¯¹æ¯æ ·æœ¬è°ƒç”¨ `prepare_latents(single_image, 1, ...)`ï¼Œæ”¶é›† `image_latents_list`, `image_ids_list`ï¼›
   - è®°å½• `image_latents_shapes = [lat.shape for lat in image_latents_list]`ï¼›
   - æ§åˆ¶åˆ†æ”¯åŒç†ï¼ˆè®¾ `type_id`ï¼‰ï¼›
   - `pad_to_max_shape` ç»Ÿä¸€å°ºå¯¸ï¼›
   - mask collate åå·²æ˜¯ `[seq]`ï¼Œç›´æ¥ä½¿ç”¨ï¼›
3. å•åˆ†è¾¨ç‡ â†’ ä¿ç•™æ—§é€»è¾‘ã€‚

### 4.2 `_pad_latents_for_multi_res`

```python
def _pad_latents_for_multi_res(latents_list, ids_list=None):
    ...
    padded_latents = torch.zeros(B, max_seq, C, ...)
    attention_mask = torch.zeros(B, max_seq, dtype=torch.bool)
    if ids_list:
        padded_ids = torch.zeros(B, max_seq, ids_dim, ...)
    ...
    if ids_list:
        return padded_latents, attention_mask, padded_ids
    return padded_latents, attention_mask
```

### 4.3 `_compute_loss_multi_resolution` ä¸ Loss å½’ä¸€åŒ–

#### 4.3.1 å½“å‰é—®é¢˜

padding åçš„ loss è®¡ç®—ä¼šåŒ…å«æ— æ•ˆ tokenï¼Œå¯¼è‡´ï¼š
- æ¢¯åº¦è¢«ç¨€é‡Šï¼ˆloss è¢« padding token çš„é›¶æ¢¯åº¦å¹³å‡ï¼‰
- ä¸åŒ batch çš„ loss scale ä¸ä¸€è‡´

#### 4.3.2 å®Œæ•´å®ç°æµç¨‹

1. æ ¹æ® `_shapes` åˆ‡ç‰‡çœŸå®åºåˆ—ï¼›
2. æ‹¼æ¥ image/control latentï¼Œè°ƒç”¨ `_pad_latents_for_multi_res`ï¼›
3. æ„é€  additive maskï¼š
   ```python
   additive_mask = latent_model_input.new_full(attention_mask.shape, 0.0)
   additive_mask[~attention_mask] = float("-inf")
   ```
4. è°ƒç”¨æ¨¡å‹ï¼š
   ```python
   model_pred = self.dit(
       hidden_states=latent_model_input.to(self.weight_dtype),
       encoder_hidden_states=prompt_embeds.to(self.weight_dtype),
       timestep=t.to(self.weight_dtype),
       img_shapes=img_shapes_per_sample,           # TODO: per-sample RoPE å®Œæˆåç”Ÿæ•ˆ
       joint_attention_kwargs={'attention_mask': additive_mask},
       ...
   )
   ```

#### 4.3.3 Loss è®¡ç®—ï¼ˆä¿®æ­£ç‰ˆï¼‰

**å…³é”®**ï¼šéœ€è¦æŒ‰å®é™…æœ‰æ•ˆ token æ•°å½’ä¸€åŒ–ï¼Œé¿å… padding ç¨€é‡Šæ¢¯åº¦ã€‚

```python
# Step 5: è®¡ç®— lossï¼ˆä¿®æ­£ç‰ˆï¼‰
if self.config.loss.get("use_mask_loss", False):
    # MaskEditLoss å†…éƒ¨å·²æŒ‰åºåˆ—é•¿åº¦å½’ä¸€åŒ–ï¼Œä½†éœ€è°ƒæ•´ä¸ºå®é™…é•¿åº¦
    loss_unreduced = self.mask_loss_fn(
        mask=mask_latent,
        model_pred=model_pred,
        target=target,
        weighting=weighting,
        reduction='none'  # è¿”å› [B, seq, C]
    )
    # ä»…ä¿ç•™çœŸå® token çš„ loss
    valid_mask = attention_mask[:, seq_txt:].unsqueeze(-1)  # [B, seq_img, 1]
    loss_masked = loss_unreduced * valid_mask.float()

    # æŒ‰å®é™… token æ•°å½’ä¸€åŒ–
    num_valid_tokens = valid_mask.sum()
    loss = loss_masked.sum() / num_valid_tokens.clamp(min=1)
else:
    # MSE loss åŒæ ·éœ€è¦ mask
    mse = F.mse_loss(model_pred, target, reduction='none')  # [B, seq, C]
    valid_mask = attention_mask[:, seq_txt:].unsqueeze(-1)
    mse_masked = mse * valid_mask.float()

    num_valid_tokens = valid_mask.sum() * mse.shape[-1]  # åŒ…å«é€šé“ç»´åº¦
    loss = mse_masked.sum() / num_valid_tokens.clamp(min=1)

# å¯é€‰ï¼šè®°å½• padding ratio ç”¨äºç›‘æ§
padding_ratio = 1.0 - (num_valid_tokens.item() / (attention_mask.shape[0] * attention_mask.shape[1]))
if padding_ratio > 0.3:  # è¶…è¿‡ 30% padding æ—¶è­¦å‘Š
    logging.debug(f"High padding ratio: {padding_ratio:.2%}")
```

**å…³é”®ç‚¹**ï¼š
- `attention_mask[:, seq_txt:]` æå–å›¾åƒéƒ¨åˆ†çš„ mask
- `num_valid_tokens.clamp(min=1)` é¿å…é™¤é›¶
- éœ€è¦ä¿®æ”¹ `MaskEditLoss` æ”¯æŒ `reduction='none'`

#### 4.3.4 å…¶å®ƒæ³¨æ„äº‹é¡¹

- cache / é cache æµç¨‹ä¿æŒä¸€è‡´ï¼›
- `MaskEditLoss` ä»…ä½œç”¨çœŸå® image tokenã€‚

### 4.4 å…¶å®ƒæ³¨æ„äº‹é¡¹

- `prepare_cached_embeddings`ï¼šæ¢å¤ `_shapes`ï¼Œå¯¹ mask åš latent åŒ–ï¼›
- `_should_use_multi_resolution` â†’ False æ—¶ä¿ç•™ legacy è·¯å¾„ï¼›
- Trainer éœ€æ ¹æ®æ¨¡å‹å±‚çš„è¿›å±•å†³å®š `img_ids` çš„ç»“æ„ï¼ˆå½“å‰ä»éœ€å…±äº«å°ºå¯¸ï¼Œå¾…æ¨¡å‹æ›´æ–°åè¾“å‡º per-sampleï¼‰ã€‚

---

## 5. æ¨¡å‹å±‚

### 5.1 FluxTransformer2DModelï¼ˆ`src/models/transformer_flux_custom.py`ï¼‰

**å·²å®Œæˆ**ï¼š

- `attention_mask` å¤„ç†ï¼ˆbool/float å…¼å®¹ã€é›¶å‡º padding tokenã€ç”Ÿæˆ additive maskï¼‰ï¼›
- å•å…ƒæµ‹è¯• `test_padding_equivalence_single_sample` / `test_attention_mask_accepts_float`ã€‚

**ä»éœ€å®Œæˆ**ï¼šper-sample RoPE / img_idsã€‚

**è®¡åˆ’ä¸å®ç°ç»†èŠ‚**ï¼š

#### 5.1.1 ä¿ç•™åŸæ ·å‚è€ƒ
`src/models/transformer_flux.py` å­˜æ”¾ diffusers åŸå®ç°ä½œä¸ºå¯¹æ¯”ã€‚

#### 5.1.2 Trainer ç«¯ img_ids ç”Ÿæˆ

åœ¨ `prepare_latents` ä¸­ä¸ºæ¯ä¸ªæ ·æœ¬ç”Ÿæˆ img_idsï¼š

     ```python
def _prepare_latent_image_ids_batched(
    self,
    shapes: List[Tuple[int, int, int]],  # [(1, H', W'), ...]
    device: torch.device,
    dtype: torch.dtype
) -> Tuple[torch.Tensor, int]:
    """ç”Ÿæˆ batched img_ids ç”¨äº per-sample RoPE

    Returns:
        img_ids: (B, max_seq, 3) - åŒ…å« batch_idx, h_idx, w_idx
        max_seq: æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆpadding åï¼‰
    """
    B = len(shapes)
    ids_list = []

    for b, (_, height, width) in enumerate(shapes):
        # ä¸ºå•ä¸ªæ ·æœ¬ç”Ÿæˆ img_ids
        ids = torch.zeros(height, width, 3, device=device, dtype=dtype)
        ids[..., 0] = b  # batch index
        ids[..., 1] = torch.arange(height, device=device, dtype=dtype)[:, None]
        ids[..., 2] = torch.arange(width, device=device, dtype=dtype)[None, :]
        ids_list.append(ids.view(-1, 3))  # (H'*W', 3)

    # Padding åˆ°æœ€å¤§é•¿åº¦
    max_seq = max(ids.shape[0] for ids in ids_list)
    padded_ids = torch.zeros(B, max_seq, 3, device=device, dtype=dtype)

    for b, ids in enumerate(ids_list):
        padded_ids[b, :ids.shape[0]] = ids

    return padded_ids, max_seq
```

#### 5.1.3 æ¨¡å‹ç«¯ per-sample RoPE

åœ¨ `FluxTransformer2DModel` ä¸­ï¼š

```python
def _compute_per_sample_rope(
    self,
    img_ids: torch.Tensor  # (B, seq, 3)
) -> List[Tuple[torch.Tensor, torch.Tensor]]:
    """ä¸ºæ¯ä¸ªæ ·æœ¬è®¡ç®—ç‹¬ç«‹çš„ RoPE é¢‘ç‡

    Returns:
        List of (txt_freqs, img_freqs) for each sample in batch
    """
    B = img_ids.shape[0]
    result = []

    for b in range(B):
        img_ids_b = img_ids[b]  # (seq, 3)

        # ä» img_ids æå–å®é™…å°ºå¯¸
        # img_ids_b[:, 1] æ˜¯ h_idxï¼Œæœ€å¤§å€¼ + 1 = height
        # img_ids_b[:, 2] æ˜¯ w_idxï¼Œæœ€å¤§å€¼ + 1 = width
        h_max = img_ids_b[:, 1].max().item() + 1
        w_max = img_ids_b[:, 2].max().item() + 1

        # ç”Ÿæˆè¯¥æ ·æœ¬çš„ RoPE é¢‘ç‡
        # ï¼ˆå…·ä½“å®ç°å‚è€ƒ diffusers çš„ rope è®¡ç®—é€»è¾‘ï¼‰
        # txt_freqs_b = ... (æ ¹æ® txt_ids è®¡ç®—)
        # img_freqs_b = ... (æ ¹æ® h_max, w_max è®¡ç®—)

        # ç¼“å­˜è¯¥å°ºå¯¸çš„é¢‘ç‡ï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
        cache_key = (b, h_max, w_max)
        if cache_key not in self._rope_cache:
            txt_freqs_b = self._compute_text_rope(...)
            img_freqs_b = self._compute_image_rope(h_max, w_max, ...)
            self._rope_cache[cache_key] = (txt_freqs_b, img_freqs_b)
        else:
            txt_freqs_b, img_freqs_b = self._rope_cache[cache_key]

        result.append((txt_freqs_b, img_freqs_b))

    return result
```

#### 5.1.4 forward ä¸­åˆ¤æ–­æ¨¡å¼

```python
def forward(self, ..., img_ids, ...):
    # åˆ¤æ–­ img_ids æ˜¯å¦åŒ…å« batch ç»´
    if img_ids is not None and img_ids.ndim == 3:
        # Per-sample mode: img_ids shape (B, seq, 3)
        image_rotary_emb = self._compute_per_sample_rope(img_ids)
         per_sample_mode = True
     else:
        # Shared mode: img_ids shape (seq, 3)
        image_rotary_emb = self._compute_shared_rope(img_ids)
         per_sample_mode = False

    # ä¼ é€’ç»™ attention blocks
    ...
     ```

#### 5.1.5 è‡ªå®šä¹‰ Attention Processor

     ```python
class FluxAttnProcessorPerSample:
    """æ”¯æŒ per-sample RoPE çš„ attention processor"""

    def __call__(
        self,
        attn,
        hidden_states,
        encoder_hidden_states,
        image_rotary_emb,  # List[(txt_freqs, img_freqs)] or Tuple
        **kwargs
    ):
        # åˆ¤æ–­æ˜¯å¦ per-sample æ¨¡å¼
        if isinstance(image_rotary_emb, list):
            # Per-sample mode
            B = hidden_states.shape[0]
            seq_len = hidden_states.shape[1]

            # å¯¹æ¯ä¸ªæ ·æœ¬å•ç‹¬åº”ç”¨ RoPE
            rotated_hidden = []
            for b in range(B):
                txt_freqs, img_freqs = image_rotary_emb[b]
                hs_b = hidden_states[b:b+1]  # (1, seq, dim)
                # åº”ç”¨ RoPE
                hs_b_rotated = apply_rope(hs_b, txt_freqs, img_freqs)
                rotated_hidden.append(hs_b_rotated)

            hidden_states = torch.cat(rotated_hidden, dim=0)
        else:
            # Shared mode - åŸé€»è¾‘
            hidden_states = apply_rope(hidden_states, *image_rotary_emb)

        # åç»­ attention è®¡ç®—ï¼ˆä¸ additive mask ååŒï¼‰
        return original_flux_attn_processor(...)
```

#### 5.1.6 å…¼å®¹æ€§ä¿è¯

- è‹¥ `img_ids` æ²¡æœ‰ batch ç»´ï¼ˆndim == 2ï¼‰ï¼Œèµ°åŸé€»è¾‘ï¼›
- å•æ ·æœ¬ inference / åŒå°ºå¯¸ batch ä¸å—å½±å“ï¼›
- ä¸ additive mask ååŒå·¥ä½œï¼ˆåŠ  -inf å±è”½ paddingï¼‰ã€‚

**TODO åˆ—è¡¨**ï¼š

- [ ] `_prepare_latent_image_ids_batched` çš„å®ç°ï¼ˆTrainer ç«¯ï¼‰ï¼›
- [ ] `_compute_per_sample_rope` çš„å®ç°ï¼ˆæ¨¡å‹ç«¯ï¼‰ï¼›
- [ ] `FluxAttnProcessorPerSample` å®ç°ï¼›
- [ ] forward ä¸­æ ¹æ®æ¨¡å¼é€‰æ‹© processorï¼›
- [ ] æ–°å¢ `test_flux_rope_per_sample`ã€`test_flux_multi_sample_padding`ï¼›
- [ ] ä¸ Trainer å¯¹æ¥ï¼ˆè¾“å‡º per-sample `img_ids` / shapesï¼‰ã€‚

### 5.2 QwenImageTransformer2DModelï¼ˆ`src/models/transformer_qwenimage.py`ï¼‰

ä¿æŒä¸Šä¸€ç‰ˆè®¡åˆ’ï¼š

- `QwenEmbedRope.forward` æ”¯æŒ per-sampleï¼›
- `QwenImageTransformer2DModel.forward` æ ¹æ® `img_shapes` è°ƒæ•´ï¼›
- `QwenDoubleStreamAttnProcessor2_0` æŒ‰æ ·æœ¬æ‹†åˆ† RoPEï¼›
- ä¸ Flux å®ç°ä¸€è‡´çš„æµ‹è¯•çŸ©é˜µã€‚

---

## 6. æµ‹è¯•è®¡åˆ’

### 6.1 å•å…ƒæµ‹è¯•çŸ©é˜µ

| Test Case | æ–‡ä»¶ | çŠ¶æ€ | è¯´æ˜ |
|-----------|------|------|------|
| **æ¨¡å‹å±‚** |
| Padding ç­‰ä»·æ€§ï¼ˆå•æ ·æœ¬ï¼‰ | `test_flux_transformer_padding.py` | âœ… | å·²å®Œæˆ |
| Float mask å…¼å®¹æ€§ | `test_flux_transformer_padding.py` | âœ… | å·²å®Œæˆ |
| Per-sample RoPE æ­£ç¡®æ€§ | `test_flux_transformer_padding.py` | â³ | **æ–°å¢** |
| æ··åˆå°ºå¯¸ forward | `test_flux_transformer_padding.py` | â³ | **æ–°å¢** |
| **Loss è®¡ç®—** |
| å¸¦ mask çš„ loss å½’ä¸€åŒ– | `tests/loss/test_mask_loss.py` | â³ | **æ–°å¢** |
| Padding token é›¶æ¢¯åº¦éªŒè¯ | `tests/loss/test_mask_loss.py` | â³ | **æ–°å¢** |
| **Trainer å±‚** |
| å¤šåˆ†è¾¨ç‡ collate | `tests/trainer/test_flux_kontext_trainer.py` | â³ | **æ–°å¢** |
| æ··åˆå°ºå¯¸è®­ç»ƒ step | `tests/trainer/test_flux_kontext_trainer.py` | â³ | **æ–°å¢** |
| åŒå°ºå¯¸ fallback | `tests/trainer/test_flux_kontext_trainer.py` | â³ | **æ–°å¢** |
| **ç¼“å­˜å…¼å®¹æ€§** |
| v1.0 ç¼“å­˜åŠ è½½ | `tests/data/test_cache_compatibility.py` | â³ | **æ–°å¢** |
| v2.0 ç¼“å­˜ä¿å­˜/åŠ è½½ | `tests/data/test_cache_compatibility.py` | â³ | **æ–°å¢** |
| **Qwen ç›¸å…³** |
| Qwen æ¨¡å‹è¿ç§»æµ‹è¯• | å¾…å®š | â³ | Flux å®Œæˆåè¿ç§» |

### 6.2 æµ‹è¯•ç”¨ä¾‹å®Œæ•´ä»£ç 

#### 6.2.1 æ¨¡å‹å±‚æµ‹è¯•

##### test_padding_equivalence_single_sampleï¼ˆå·²å®Œæˆï¼‰

```python
def test_padding_equivalence_single_sample():
    """éªŒè¯å•æ ·æœ¬ padding ç­‰ä»·æ€§"""
    torch.manual_seed(0)
    device = torch.device("cpu")

    model = _build_model().to(device)
    model.eval()

    batch_size = 1
    seq_txt = 4
    height, width = 2, 4  # seq_img = 8
    seq_img = height * width
    pad_tokens = 4

    hidden_states = torch.randn(batch_size, seq_img, 64, device=device)
    encoder_hidden_states = torch.randn(batch_size, seq_txt, 32, device=device)
    pooled = torch.randn(batch_size, 16, device=device)
    timestep = torch.randint(0, 1000, (batch_size,), device=device)
    img_ids = _prepare_latent_image_ids(height, width, device, torch.float32)
    txt_ids = torch.stack(
        [torch.tensor([0.0, float(i), 0.0], device=device) for i in range(seq_txt)],
        dim=0,
    )

    # Baseline: æ—  padding
    with torch.no_grad():
        out_base = model(
            hidden_states=hidden_states,
            encoder_hidden_states=encoder_hidden_states,
            pooled_projections=pooled,
            timestep=timestep,
            img_ids=img_ids,
            txt_ids=txt_ids,
        ).sample

    # æ·»åŠ  padding
    hidden_states_padded = torch.cat(
        [hidden_states, torch.zeros(batch_size, pad_tokens, 64, device=device)], dim=1
    )
    extra_ids = torch.zeros(pad_tokens, 3, device=device)
    extra_ids[:, 1] = height + torch.arange(pad_tokens, device=device)
    img_ids_padded = torch.cat([img_ids, extra_ids], dim=0)

    mask = torch.ones(batch_size, seq_txt + seq_img + pad_tokens, device=device, dtype=torch.bool)
    mask[:, seq_txt + seq_img :] = False

    with torch.no_grad():
        out_padded = model(
            hidden_states=hidden_states_padded,
            encoder_hidden_states=encoder_hidden_states,
            pooled_projections=pooled,
            timestep=timestep,
            img_ids=img_ids_padded,
            txt_ids=txt_ids,
            attention_mask=mask,
        ).sample

    # éªŒè¯ï¼šé padding éƒ¨åˆ†åº”è¯¥ä¸€è‡´
    assert torch.allclose(out_base, out_padded[:, :seq_img], atol=1e-5)
```

##### test_attention_mask_accepts_floatï¼ˆå·²å®Œæˆï¼‰

```python
def test_attention_mask_accepts_float():
    """éªŒè¯ attention_mask æ”¯æŒ float å’Œ bool"""
    torch.manual_seed(1)
    device = torch.device("cpu")

    model = _build_model().to(device)
    model.eval()

    batch_size = 1
    seq_txt = 2
    height, width = 2, 2
    seq_img = height * width

    hidden_states = torch.randn(batch_size, seq_img, 64, device=device)
    encoder_hidden_states = torch.randn(batch_size, seq_txt, 32, device=device)
    pooled = torch.randn(batch_size, 16, device=device)
    timestep = torch.randint(0, 1000, (batch_size,), device=device)
    img_ids = _prepare_latent_image_ids(height, width, device, torch.float32)
    txt_ids = torch.stack(
        [torch.tensor([0.0, float(i), 0.0], device=device) for i in range(seq_txt)],
        dim=0,
    )

    hidden_states_padded = torch.cat(
        [hidden_states, torch.zeros(batch_size, 2, 64, device=device)], dim=1
    )
    img_ids_padded = torch.cat([img_ids, torch.zeros(2, 3, device=device)], dim=0)

    mask_float = torch.ones(batch_size, seq_txt + seq_img + 2, device=device)
    mask_float[:, seq_txt + seq_img :] = 0.0
    mask_bool = mask_float.bool()

    with torch.no_grad():
        out_float = model(
            hidden_states=hidden_states_padded,
            encoder_hidden_states=encoder_hidden_states,
            pooled_projections=pooled,
            timestep=timestep,
            img_ids=img_ids_padded,
            txt_ids=txt_ids,
            attention_mask=mask_float,
        ).sample

        out_bool = model(
            hidden_states=hidden_states_padded,
            encoder_hidden_states=encoder_hidden_states,
            pooled_projections=pooled,
            timestep=timestep,
            img_ids=img_ids_padded,
            txt_ids=txt_ids,
            attention_mask=mask_bool,
        ).sample

    # éªŒè¯ï¼šfloat å’Œ bool mask ç»“æœä¸€è‡´
    assert torch.allclose(out_float, out_bool, atol=1e-5)
```

##### test_flux_rope_per_sampleï¼ˆæ–°å¢ï¼‰

```python
def test_flux_rope_per_sample():
    """éªŒè¯ä¸åŒå°ºå¯¸æ ·æœ¬çš„ RoPE è®¡ç®—æ­£ç¡®æ€§"""
    torch.manual_seed(2)
    device = torch.device("cpu")

    model = _build_model().to(device)
    model.eval()

    # ä¸¤ä¸ªä¸åŒå°ºå¯¸çš„æ ·æœ¬
    batch_size = 2
    seq_txt = 3
    shapes = [(1, 2, 4), (1, 3, 3)]  # seq_len = 8 å’Œ 9
    max_seq = 9

    # ç”Ÿæˆ batched img_ids (B, seq, 3)
    img_ids_batched = torch.zeros(batch_size, max_seq, 3, device=device)
    for b, (_, h, w) in enumerate(shapes):
        ids = torch.zeros(h, w, 3, device=device)
        ids[..., 0] = b
        ids[..., 1] = torch.arange(h, device=device)[:, None]
        ids[..., 2] = torch.arange(w, device=device)[None, :]
        img_ids_batched[b, :h*w] = ids.view(-1, 3)

    # ç”Ÿæˆ txt_ids (shared)
    txt_ids = torch.stack(
        [torch.tensor([0.0, float(i), 0.0], device=device) for i in range(seq_txt)],
        dim=0,
    )

    # Hidden states å’Œ mask
    hidden_states = torch.randn(batch_size, max_seq, 64, device=device)
    encoder_hidden_states = torch.randn(batch_size, seq_txt, 32, device=device)
    pooled = torch.randn(batch_size, 16, device=device)
    timestep = torch.randint(0, 1000, (batch_size,), device=device)

    attention_mask = torch.zeros(batch_size, seq_txt + max_seq, dtype=torch.bool, device=device)
    attention_mask[0, :seq_txt + 8] = True  # ç¬¬ä¸€ä¸ªæ ·æœ¬ 8 ä¸ª token
    attention_mask[1, :seq_txt + 9] = True  # ç¬¬äºŒä¸ªæ ·æœ¬ 9 ä¸ª token

    with torch.no_grad():
        output = model(
            hidden_states=hidden_states,
            encoder_hidden_states=encoder_hidden_states,
            pooled_projections=pooled,
            timestep=timestep,
            img_ids=img_ids_batched,  # (B, seq, 3) è§¦å‘ per-sample mode
            txt_ids=txt_ids,
            attention_mask=attention_mask,
        )

    # éªŒè¯ 1: padding éƒ¨åˆ†åº”è¯¥å…¨é›¶
    assert torch.allclose(output.sample[0, 8:], torch.zeros_like(output.sample[0, 8:]), atol=1e-5)

    # éªŒè¯ 2: è¾“å‡º shape æ­£ç¡®
    assert output.sample.shape == (batch_size, max_seq, 64)

    # éªŒè¯ 3: é padding éƒ¨åˆ†ä¸å…¨ä¸ºé›¶
    assert not torch.allclose(output.sample[0, :8], torch.zeros_like(output.sample[0, :8]))
    assert not torch.allclose(output.sample[1, :9], torch.zeros_like(output.sample[1, :9]))
```

##### test_flux_multi_sample_paddingï¼ˆæ–°å¢ï¼‰

```python
def test_flux_multi_sample_padding():
    """éªŒè¯æ··åˆå°ºå¯¸ batch çš„ forward æ­£ç¡®æ€§"""
    torch.manual_seed(3)
    device = torch.device("cpu")

    model = _build_model().to(device)
    model.eval()

    batch_size = 3
    seq_txt = 2
    shapes = [(1, 2, 2), (1, 3, 4), (1, 2, 3)]  # seq_len = 4, 12, 6
    max_seq = 12

    # æ„é€  batched è¾“å…¥
    img_ids_batched = torch.zeros(batch_size, max_seq, 3, device=device)
    hidden_states = torch.randn(batch_size, max_seq, 64, device=device)
    attention_mask = torch.zeros(batch_size, seq_txt + max_seq, dtype=torch.bool, device=device)

    for b, (_, h, w) in enumerate(shapes):
        seq_len = h * w
        ids = torch.zeros(h, w, 3, device=device)
        ids[..., 0] = b
        ids[..., 1] = torch.arange(h, device=device)[:, None]
        ids[..., 2] = torch.arange(w, device=device)[None, :]
        img_ids_batched[b, :seq_len] = ids.view(-1, 3)
        attention_mask[b, :seq_txt + seq_len] = True

    txt_ids = torch.stack(
        [torch.tensor([0.0, float(i), 0.0], device=device) for i in range(seq_txt)],
        dim=0,
    )
    encoder_hidden_states = torch.randn(batch_size, seq_txt, 32, device=device)
    pooled = torch.randn(batch_size, 16, device=device)
    timestep = torch.randint(0, 1000, (batch_size,), device=device)

    with torch.no_grad():
        output = model(
            hidden_states=hidden_states,
            encoder_hidden_states=encoder_hidden_states,
            pooled_projections=pooled,
            timestep=timestep,
            img_ids=img_ids_batched,
            txt_ids=txt_ids,
            attention_mask=attention_mask,
        )

    # éªŒè¯ï¼šæ¯ä¸ªæ ·æœ¬çš„ padding éƒ¨åˆ†å…¨é›¶
    assert torch.allclose(output.sample[0, 4:], torch.zeros_like(output.sample[0, 4:]), atol=1e-5)
    assert torch.allclose(output.sample[2, 6:], torch.zeros_like(output.sample[2, 6:]), atol=1e-5)
    # æ ·æœ¬ 1 æ²¡æœ‰ padding

    # éªŒè¯ï¼šé padding éƒ¨åˆ†æœ‰å€¼
    assert output.sample[0, :4].abs().sum() > 0
    assert output.sample[1, :12].abs().sum() > 0
    assert output.sample[2, :6].abs().sum() > 0
```

#### 6.2.2 Loss è®¡ç®—æµ‹è¯•

##### test_mask_loss_with_reduction_noneï¼ˆæ–°å¢ï¼‰

```python
def test_mask_loss_with_reduction_none():
    """éªŒè¯ MaskEditLoss æ”¯æŒ reduction='none'"""
    from src.loss.edit_mask_loss import MaskEditLoss

    loss_fn = MaskEditLoss(foreground_weight=2.0, background_weight=1.0)

    B, seq, C = 2, 10, 64
    mask = torch.rand(B, seq) > 0.5  # éšæœº mask
    pred = torch.randn(B, seq, C)
    target = torch.randn(B, seq, C)

    # æµ‹è¯• reduction='none'
    loss_unreduced = loss_fn(mask, pred, target, reduction='none')

    # éªŒè¯ shape
    assert loss_unreduced.shape == (B, seq, C)

    # éªŒè¯ä¸æ‰‹åŠ¨è®¡ç®—ä¸€è‡´
    element_loss = (pred - target) ** 2
    weight_mask = (mask.float() * 2.0 + (~mask).float() * 1.0).unsqueeze(-1)
    expected = element_loss * weight_mask

    assert torch.allclose(loss_unreduced, expected, atol=1e-5)
```

##### test_loss_normalization_with_paddingï¼ˆæ–°å¢ï¼‰

```python
def test_loss_normalization_with_padding():
    """éªŒè¯ padding æƒ…å†µä¸‹ loss æ­£ç¡®å½’ä¸€åŒ–"""
    import torch.nn.functional as F

    # ä¸¤ä¸ªæ ·æœ¬ï¼š8 å’Œ 6 ä¸ªæœ‰æ•ˆ token
    B, max_seq, C = 2, 10, 64
    seq_txt = 2

    pred = torch.randn(B, max_seq, C)
    target = torch.randn(B, max_seq, C)

    # Attention mask: [B, seq_txt + max_seq]
    attention_mask = torch.zeros(B, seq_txt + max_seq, dtype=torch.bool)
    attention_mask[0, :seq_txt + 8] = True  # æ ·æœ¬ 0: 8 ä¸ªæœ‰æ•ˆ image token
    attention_mask[1, :seq_txt + 6] = True  # æ ·æœ¬ 1: 6 ä¸ªæœ‰æ•ˆ image token

    # æå– image éƒ¨åˆ†çš„ mask
    valid_mask = attention_mask[:, seq_txt:].unsqueeze(-1)  # [B, max_seq, 1]

    # è®¡ç®— masked MSE loss
    mse = F.mse_loss(pred, target, reduction='none')  # [B, max_seq, C]
    mse_masked = mse * valid_mask.float()
    num_valid_tokens = valid_mask.sum() * C
    loss = mse_masked.sum() / num_valid_tokens.clamp(min=1)

    # æ‰‹åŠ¨éªŒè¯
    valid_elements_0 = pred[0, :8] - target[0, :8]  # æ ·æœ¬ 0
    valid_elements_1 = pred[1, :6] - target[1, :6]  # æ ·æœ¬ 1
    expected = (valid_elements_0 ** 2).sum() + (valid_elements_1 ** 2).sum()
    expected = expected / ((8 + 6) * C)

    assert torch.allclose(loss, expected, atol=1e-5)

    # éªŒè¯ï¼šå¦‚æœä¸åšå½’ä¸€åŒ–ï¼Œloss ä¼šè¢«ç¨€é‡Š
    naive_loss = mse.mean()
    assert naive_loss < loss  # naive loss è¢« padding ç¨€é‡Š
```

##### test_padding_token_zero_gradientï¼ˆæ–°å¢ï¼‰

```python
def test_padding_token_zero_gradient():
    """éªŒè¯ padding token çš„æ¢¯åº¦ä¸ºé›¶"""
    B, max_seq, C = 2, 10, 64
    seq_txt = 2

    pred = torch.randn(B, max_seq, C, requires_grad=True)
    target = torch.randn(B, max_seq, C)

    attention_mask = torch.zeros(B, seq_txt + max_seq, dtype=torch.bool)
    attention_mask[0, :seq_txt + 8] = True
    attention_mask[1, :seq_txt + 6] = True

    valid_mask = attention_mask[:, seq_txt:].unsqueeze(-1)

    # è®¡ç®— loss
    mse = (pred - target) ** 2
    mse_masked = mse * valid_mask.float()
    loss = mse_masked.sum() / valid_mask.sum().clamp(min=1)

    # åå‘ä¼ æ’­
    loss.backward()

    # éªŒè¯ï¼špadding ä½ç½®çš„æ¢¯åº¦åº”è¯¥ä¸ºé›¶
    assert torch.allclose(pred.grad[0, 8:], torch.zeros_like(pred.grad[0, 8:]), atol=1e-6)
    assert torch.allclose(pred.grad[1, 6:], torch.zeros_like(pred.grad[1, 6:]), atol=1e-6)

    # éªŒè¯ï¼šé padding ä½ç½®æœ‰æ¢¯åº¦
    assert pred.grad[0, :8].abs().sum() > 0
    assert pred.grad[1, :6].abs().sum() > 0
```

#### 6.2.3 Trainer å±‚æµ‹è¯•

##### test_should_use_multi_resolution_modeï¼ˆæ–°å¢ï¼‰

```python
def test_should_use_multi_resolution_mode():
    """éªŒè¯ _should_use_multi_resolution_mode åˆ¤æ–­é€»è¾‘"""
    from src.trainer.flux_kontext_trainer import FluxKontextLoraTrainer

    # Mock config
    config = type('Config', (), {})()
    trainer = FluxKontextLoraTrainer(config)

    # Case 1: batch_size == 1 â†’ False
    batch = {"prompt_embeds": torch.randn(1, 128, 768)}
    assert not trainer._should_use_multi_resolution_mode(batch)

    # Case 2: æ‰€æœ‰æ ·æœ¬åŒå°ºå¯¸ â†’ False
    batch = {
        "prompt_embeds": torch.randn(4, 128, 768),
        "image_latents_shapes": [[(1, 32, 64)]] * 4,  # å…¨éƒ¨ç›¸åŒ
    }
    assert not trainer._should_use_multi_resolution_mode(batch)

    # Case 3: ä¸åŒå°ºå¯¸ â†’ True
    batch = {
        "prompt_embeds": torch.randn(4, 128, 768),
        "image_latents_shapes": [[(1, 32, 64)], [(1, 48, 48)], [(1, 32, 64)], [(1, 40, 50)]],
    }
    assert trainer._should_use_multi_resolution_mode(batch)

    # Case 4: height/width fallback
    batch = {
        "prompt_embeds": torch.randn(3, 128, 768),
        "height": [512, 640, 512],
        "width": [512, 640, 768],
    }
    assert trainer._should_use_multi_resolution_mode(batch)
```

##### test_pad_latents_for_multi_resï¼ˆæ–°å¢ï¼‰

```python
def test_pad_latents_for_multi_res():
    """éªŒè¯ latents padding å‡½æ•°"""
    from src.trainer.flux_kontext_trainer import FluxKontextLoraTrainer

    config = type('Config', (), {})()
    trainer = FluxKontextLoraTrainer(config)

    # ä¸‰ä¸ªä¸åŒé•¿åº¦çš„ latents
    latents_list = [
        torch.randn(1, 8, 16),   # seq=8
        torch.randn(1, 12, 16),  # seq=12
        torch.randn(1, 6, 16),   # seq=6
    ]

    # å¯¹åº”çš„ ids
    ids_list = [
        torch.randn(8, 3),
        torch.randn(12, 3),
        torch.randn(6, 3),
    ]

    # Padding
    padded_latents, attention_mask, padded_ids = trainer._pad_latents_for_multi_res(
        latents_list, ids_list
    )

    # éªŒè¯ shape
    assert padded_latents.shape == (3, 12, 16)  # max_seq = 12
    assert attention_mask.shape == (3, 12)
    assert padded_ids.shape == (3, 12, 3)

    # éªŒè¯ mask
    assert attention_mask[0, :8].all() and not attention_mask[0, 8:].any()
    assert attention_mask[1, :12].all()
    assert attention_mask[2, :6].all() and not attention_mask[2, 6:].any()

    # éªŒè¯åŸå§‹å€¼ä¿ç•™
    assert torch.allclose(padded_latents[0, :8], latents_list[0].squeeze(0))
    assert torch.allclose(padded_latents[1, :12], latents_list[1].squeeze(0))
    assert torch.allclose(padded_ids[2, :6], ids_list[2])

    # éªŒè¯ padding ä¸ºé›¶
    assert torch.allclose(padded_latents[0, 8:], torch.zeros_like(padded_latents[0, 8:]))
```

##### test_mixed_resolution_training_stepï¼ˆæ–°å¢ï¼‰

```python
def test_mixed_resolution_training_step():
    """éªŒè¯æ··åˆåˆ†è¾¨ç‡è®­ç»ƒçš„å®Œæ•´ step"""
    import os
    from unittest import mock

    os.environ.setdefault("HF_TOKEN", "dummy")
    with mock.patch("huggingface_hub.login"):
        from src.trainer.flux_kontext_trainer import FluxKontextLoraTrainer

    # æ„é€  minimal config
    config = type('Config', (), {
        'model': type('Model', (), {
            'pretrained_model_name_or_path': 'black-forest-labs/FLUX.1-Kontext',
        })(),
        'data': {},
        'loss': {'use_mask_loss': False},
    })()

    trainer = FluxKontextLoraTrainer(config)
    trainer.setup()  # åŠ è½½æ¨¡å‹

    # æ„é€ æ··åˆå°ºå¯¸ batch
    batch = {
        "image": torch.randn(2, 3, 512, 512),  # ä¼šè¢«å¤„ç†æˆä¸åŒå°ºå¯¸
        "prompt": ["a cat", "a dog"],
        "image_latents_shapes": [[(1, 32, 64)], [(1, 48, 48)]],
        "prompt_embeds": torch.randn(2, 77, 768),
        "pooled_prompt_embeds": torch.randn(2, 768),
    }

    # æ‰§è¡Œè®­ç»ƒ step
    trainer.train()
    loss = trainer.training_step(batch, 0)

    # éªŒè¯ loss æ˜¯æ ‡é‡ä¸”å¯åå‘ä¼ æ’­
    assert isinstance(loss, torch.Tensor)
    assert loss.dim() == 0
    assert not torch.isnan(loss)
    assert not torch.isinf(loss)

    # éªŒè¯æ¢¯åº¦å¯ä»¥ä¼ æ’­
    loss.backward()
    # æ£€æŸ¥æ¨¡å‹å‚æ•°æœ‰æ¢¯åº¦
    has_grad = any(p.grad is not None for p in trainer.dit.parameters() if p.requires_grad)
    assert has_grad
```

##### test_same_resolution_fallbackï¼ˆæ–°å¢ï¼‰

```python
def test_same_resolution_fallback():
    """éªŒè¯åŒå°ºå¯¸ batch è‡ªåŠ¨å›é€€åˆ° shared mode"""
    from src.trainer.flux_kontext_trainer import FluxKontextLoraTrainer

    config = type('Config', (), {})()
    trainer = FluxKontextLoraTrainer(config)

    # æ‰€æœ‰æ ·æœ¬åŒå°ºå¯¸
    batch = {
        "prompt_embeds": torch.randn(4, 128, 768),
        "image_latents_shapes": [[(1, 32, 64)]] * 4,
        "height": [512] * 4,
        "width": [1024] * 4,
    }

    # åº”è¯¥ä½¿ç”¨ shared mode
    assert not trainer._should_use_multi_resolution_mode(batch)

    # prepare_embeddings åº”è¯¥èµ°åŸé€»è¾‘ï¼ˆä¸åš paddingï¼‰
    # è¿™éœ€è¦ mock ä¸€äº›æ–¹æ³•ï¼Œè¿™é‡ŒåªéªŒè¯åˆ¤æ–­é€»è¾‘
```

#### 6.2.4 ç¼“å­˜å…¼å®¹æ€§æµ‹è¯•

##### test_cache_v1_loadingï¼ˆæ–°å¢ï¼‰

```python
def test_cache_v1_loading():
    """éªŒè¯ v1.0 ç¼“å­˜åŠ è½½å…¼å®¹æ€§"""
    from src.trainer.flux_kontext_trainer import FluxKontextLoraTrainer

    config = type('Config', (), {
        'data': {'default_height': 512, 'default_width': 512}
    })()
    trainer = FluxKontextLoraTrainer(config)
    trainer.vae_scale_factor = 8

    # æ¨¡æ‹Ÿ v1.0 ç¼“å­˜ï¼ˆæ²¡æœ‰ version å’Œ img_shapesï¼‰
    batch_v1 = {
        "prompt_embeds": torch.randn(2, 77, 768),
        "pooled_prompt_embeds": torch.randn(2, 768),
        "height": 512,
        "width": 512,
        # æ³¨æ„ï¼šæ²¡æœ‰ "version" å’Œ "img_shapes"
    }

    # åŠ è½½å…¼å®¹å¤„ç†
    trainer.prepare_cached_embeddings(batch_v1)

    # éªŒè¯ï¼šè‡ªåŠ¨é‡å»º img_shapes
    assert "image_latents_shapes" in batch_v1
    shapes = batch_v1["image_latents_shapes"]
    assert len(shapes) == 2  # batch_size = 2

    # éªŒè¯å°ºå¯¸è®¡ç®—æ­£ç¡®
    expected_h = 512 // 8 // 2  # 32
    expected_w = 512 // 8 // 2  # 32
    assert shapes[0] == [(1, expected_h, expected_w)]
    assert shapes[1] == [(1, expected_h, expected_w)]
```

##### test_cache_v2_save_and_loadï¼ˆæ–°å¢ï¼‰

```python
def test_cache_v2_save_and_load():
    """éªŒè¯ v2.0 ç¼“å­˜ä¿å­˜å’ŒåŠ è½½"""
    import tempfile
    import os

    from src.data.dataset import ImageEditDataset

    # æ„é€ æ•°æ®
    cache_data = {
        "version": "2.0",
        "prompt_embeds": torch.randn(5, 77, 768),
        "pooled_prompt_embeds": torch.randn(5, 768),
        "img_shapes": torch.tensor([
            (1, 32, 64),
            (1, 48, 48),
            (1, 40, 50),
            (1, 32, 64),
            (1, 36, 54),
        ]),
    }

    # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(delete=False, suffix='.pt') as f:
        cache_path = f.name
        torch.save(cache_data, cache_path)

    try:
        # åŠ è½½
        loaded = torch.load(cache_path)

        # éªŒè¯ç‰ˆæœ¬
        assert loaded["version"] == "2.0"

        # éªŒè¯ img_shapes
        assert loaded["img_shapes"].shape == (5, 3)
        assert torch.equal(loaded["img_shapes"][0], torch.tensor([1, 32, 64]))
        assert torch.equal(loaded["img_shapes"][1], torch.tensor([1, 48, 48]))

        # éªŒè¯å…¶ä»–å­—æ®µ
        assert loaded["prompt_embeds"].shape == (5, 77, 768)
        assert loaded["pooled_prompt_embeds"].shape == (5, 768)
    finally:
        os.unlink(cache_path)
```

##### test_cache_migrationï¼ˆæ–°å¢ï¼‰

```python
def test_cache_migration():
    """éªŒè¯ç¼“å­˜è¿ç§»å·¥å…·"""
    import tempfile
    import os

    # æ¨¡æ‹Ÿ v1.0 ç¼“å­˜
    cache_v1 = {
        "prompt_embeds": torch.randn(3, 77, 768),
        "height": 512,
        "width": 1024,
    }

    # ä¿å­˜ v1
    with tempfile.NamedTemporaryFile(delete=False, suffix='_v1.pt') as f:
        cache_v1_path = f.name
        torch.save(cache_v1, cache_v1_path)

    with tempfile.NamedTemporaryFile(delete=False, suffix='_v2.pt') as f:
        cache_v2_path = f.name

    try:
        # æ‰§è¡Œè¿ç§»
        def migrate_cache(old_path, new_path):
            data = torch.load(old_path)
            data["version"] = "2.0"

            if "img_shapes" not in data:
                height = data.get("height", 512)
                width = data.get("width", 512)
                latent_h = height // 8 // 2
                latent_w = width // 8 // 2
                batch_size = data["prompt_embeds"].shape[0]
                data["img_shapes"] = torch.tensor(
                    [(1, latent_h, latent_w)] * batch_size
                )

            torch.save(data, new_path)

        migrate_cache(cache_v1_path, cache_v2_path)

        # éªŒè¯è¿ç§»ç»“æœ
        cache_v2 = torch.load(cache_v2_path)
        assert cache_v2["version"] == "2.0"
        assert "img_shapes" in cache_v2
        assert cache_v2["img_shapes"].shape == (3, 3)

        # éªŒè¯å°ºå¯¸è®¡ç®—
        expected_h = 512 // 8 // 2  # 32
        expected_w = 1024 // 8 // 2  # 64
        assert torch.equal(cache_v2["img_shapes"][0], torch.tensor([1, expected_h, expected_w]))
    finally:
        os.unlink(cache_v1_path)
        if os.path.exists(cache_v2_path):
            os.unlink(cache_v2_path)
```

### 6.3 æµ‹è¯•ä¼˜å…ˆçº§

**é«˜ä¼˜å…ˆçº§ï¼ˆå¿…é¡»é€šè¿‡ï¼‰**ï¼š
- âœ… å•æ ·æœ¬ç­‰ä»·æ€§ï¼ˆå·²æœ‰ï¼‰
- ğŸ”´ Per-sample RoPE æ­£ç¡®æ€§
- ğŸ”´ Loss å½’ä¸€åŒ–æ­£ç¡®æ€§
- ğŸ”´ ç¼“å­˜å…¼å®¹æ€§

**ä¸­ä¼˜å…ˆçº§ï¼ˆé‡è¦ä½†éé˜»å¡ï¼‰**ï¼š
- ğŸŸ¡ æ··åˆå°ºå¯¸æ¢¯åº¦ä¼ æ’­
- ğŸŸ¡ åŒå°ºå¯¸ fallback

**ä½ä¼˜å…ˆçº§ï¼ˆå¯é€‰ï¼‰**ï¼š
- âšª æç«¯å°ºå¯¸æ¯”ä¾‹æµ‹è¯•
- âšª æ€§èƒ½åŸºå‡†æµ‹è¯•

---

## 7. å…¶å®ƒ Trainer è¿ç§»

| Trainer | æ”¹åŠ¨è¦ç‚¹ | è¯´æ˜ |
|---------|----------|------|
| `src/trainer/qwen_image_edit_trainer.py` | å¤ç”¨ `_should_use_multi_resolution_mode`ã€`_pad_latents_for_multi_res`ã€`_compute_loss_multi_resolution`ï¼›ç¼“å­˜æ¨¡å¼è½¬å› list-of-shapesï¼›mask latent åŒ– | `_get_image_shapes` éœ€æ‰©å±• per-sampleï¼›cache / é cache ä¿æŒä¸€è‡´ |
| `src/trainer/qwen_image_edit_plus_trainer.py` | åŒä¸Šï¼Œé¢å¤–å¤„ç†å¤šæ§åˆ¶åˆ†æ”¯ | `n_controls`, `height_control_i`, `width_control_i` å¿…é¡»å‚ä¸ |

æŠ½è±¡å»ºè®®ï¼š

- å°† `_should_use_multi_resolution` ç­‰å·¥å…·æå‡åˆ° BaseTrainer æˆ– utilï¼›
- Flux ç‰ˆæœ¬å°±ç»ªåå†è¿ç§» Qwen ç³»åˆ—ã€‚

---

## 8. ä¸‹ä¸€æ­¥ Checklist

æŒ‰ä¾èµ–å…³ç³»æ’åºçš„å®æ–½è®¡åˆ’ï¼š

### Phase 1: åŸºç¡€è®¾æ–½ï¼ˆ1-2 å¤©ï¼‰
1. æ·»åŠ ç±»å‹å®šä¹‰å’Œæ•°æ®æ ¼å¼æ–‡æ¡£ï¼ˆSection 0.1, 0.2ï¼‰
2. å®ç° `_should_use_multi_resolution_mode`ï¼ˆSection 4.1.1ï¼‰
3. æ›´æ–° Loss è®¡ç®—é€»è¾‘ï¼Œæ”¯æŒ `reduction='none'`ï¼ˆSection 4.3.3ï¼‰
4. å®ç°ç¼“å­˜ç‰ˆæœ¬ç®¡ç†ï¼ˆSection 3.1ï¼‰

### Phase 2: æ¨¡å‹å±‚ï¼ˆ2-3 å¤©ï¼‰
5. å®ç° `_prepare_latent_image_ids_batched`ï¼ˆSection 5.1.2ï¼‰
6. å®ç° `_compute_per_sample_rope`ï¼ˆSection 5.1.3ï¼‰
7. å®ç° `FluxAttnProcessorPerSample`ï¼ˆSection 5.1.5ï¼‰
8. å•å…ƒæµ‹è¯•ï¼š`test_flux_rope_per_sample`ã€`test_flux_multi_sample_padding`

### Phase 3: Trainer é›†æˆï¼ˆ1-2 å¤©ï¼‰
9. æ›´æ–° `prepare_embeddings` æ”¯æŒ per-sample è·¯å¾„
10. å®ç° `_pad_latents_for_multi_res`
11. æ›´æ–° `_compute_loss_multi_resolution`
12. æµ‹è¯•ï¼š`test_training_with_mixed_resolutions`

### Phase 4: éªŒè¯ä¸ä¼˜åŒ–ï¼ˆ1-2 å¤©ï¼‰
13. ç«¯åˆ°ç«¯æµ‹è¯•ï¼ˆæ•°æ® â†’ è®­ç»ƒ â†’ æ¨ç†ï¼‰
14. æ€§èƒ½åŸºå‡†æµ‹è¯•
15. æ–‡æ¡£æ›´æ–°ä¸ Migration Guide
16. Code review ä¸é‡æ„

**æ€»è®¡ï¼šçº¦ 6-9 å¤©**

---

## 9. Migration Guideï¼ˆè¿ç§»æŒ‡å—ï¼‰

### 9.1 ä»å•åˆ†è¾¨ç‡è¿ç§»

#### 9.1.1 è¿ç§»æ­¥éª¤

1. **æ›´æ–°é…ç½®æ–‡ä»¶**ï¼šæ·»åŠ  `multi_resolutions` å³è‡ªåŠ¨å¯ç”¨å¤šåˆ†è¾¨ç‡

```yaml
# æ—§é…ç½®ï¼ˆå•åˆ†è¾¨ç‡ï¼‰
data:
  init_args:
    processor:
      init_args:
        target_pixels: 262144  # 512*512

# æ–°é…ç½®ï¼ˆå¤šåˆ†è¾¨ç‡ï¼‰
data:
  init_args:
    processor:
      init_args:
        # å­˜åœ¨ multi_resolutions å³è‡ªåŠ¨å¯ç”¨å¤šåˆ†è¾¨ç‡
        multi_resolutions:
          - "512*512"
          - "640*640"
          - "768*512"
```

2. **é‡æ–°ç”Ÿæˆç¼“å­˜**ï¼ˆæ¨èï¼‰æˆ–ä½¿ç”¨å…¼å®¹æ¨¡å¼

3. **éªŒè¯è®­ç»ƒ**ï¼šå…ˆç”¨ batch_size=1 æµ‹è¯•å•æ ·æœ¬ä¸€è‡´æ€§

4. **é€æ­¥å¢åŠ  batch_size**

#### 9.1.2 éªŒè¯æ£€æŸ¥ç‚¹

```bash
# 1. å•æ ·æœ¬æ¨ç†ä¸€è‡´æ€§
python -m src.main --config configs/test.yaml --mode predict --batch_size 1

# 2. æ··åˆåˆ†è¾¨ç‡è®­ç»ƒ
python -m src.main --config configs/test.yaml --mode fit --batch_size 4

# 3. æ£€æŸ¥ loss æ˜¯å¦åˆç†
# - Loss ä¸åº”å›  batch ä¸­å°ºå¯¸ç»„åˆè€Œå‰§çƒˆæ³¢åŠ¨
# - ç›¸åŒæ ·æœ¬åœ¨ä¸åŒ batch ä¸­çš„ loss åº”æ¥è¿‘
```

### 9.2 ç¼“å­˜æ•°æ®å¤„ç†

#### é€‰é¡¹ 1ï¼šé‡æ–°ç”Ÿæˆï¼ˆæ¨èï¼‰

```bash
python -m src.main --config configs/xxx.yaml --mode cache
```

**ä¼˜ç‚¹**ï¼šå®Œå…¨å…¼å®¹æ–°æ ¼å¼ï¼Œæ€§èƒ½æœ€ä½³
**ç¼ºç‚¹**ï¼šéœ€è¦é‡æ–°å¤„ç†æ‰€æœ‰æ•°æ®ï¼ˆè€—æ—¶ï¼‰

#### é€‰é¡¹ 2ï¼šä½¿ç”¨å…¼å®¹æ¨¡å¼

- ä»£ç ä¼šè‡ªåŠ¨æ£€æµ‹ v1.0 ç¼“å­˜
- è¿è¡Œæ—¶é‡å»º `img_shapes`
- æ€§èƒ½è½»å¾®ä¸‹é™ï¼ˆéœ€åŠ¨æ€è®¡ç®—ï¼‰

**ä¼˜ç‚¹**ï¼šæ— éœ€é‡æ–°ç”Ÿæˆç¼“å­˜
**ç¼ºç‚¹**ï¼šè¿è¡Œæ—¶å¼€é”€ï¼Œä¸æ”¯æŒçœŸæ­£çš„å¤šåˆ†è¾¨ç‡

#### é€‰é¡¹ 3ï¼šæ‰¹é‡è¿ç§»

```bash
python scripts/migrate_cache_v1_to_v2.py \
    --input_dir data/cache/v1/ \
    --output_dir data/cache/v2/
```

**ä¼˜ç‚¹**ï¼šä¿ç•™ç¼“å­˜æ•°æ®ï¼Œæ”¯æŒå¤šåˆ†è¾¨ç‡
**ç¼ºç‚¹**ï¼šéœ€è¦æ‰‹åŠ¨æ‰§è¡Œè¿ç§»è„šæœ¬

### 9.3 å¸¸è§é—®é¢˜æ’æŸ¥

#### Q1: Loss çªç„¶å˜å¤§æˆ–å‡ºç° NaN

**å¯èƒ½åŸå› **ï¼š
- Padding token å‚ä¸äº† loss è®¡ç®—
- Loss å½’ä¸€åŒ–ä¸æ­£ç¡®

**è§£å†³æ–¹æ³•**ï¼š
```python
# æ£€æŸ¥ attention_mask æ˜¯å¦æ­£ç¡®ä¼ é€’
logging.info(f"attention_mask sum: {attention_mask.sum()}")

# ç¡®è®¤ loss æŒ‰æœ‰æ•ˆ token å½’ä¸€åŒ–
num_valid = attention_mask.sum()
logging.info(f"Valid tokens: {num_valid}, Total: {attention_mask.numel()}")
```

#### Q2: æ˜¾å­˜å ç”¨æ˜¾è‘—å¢åŠ 

**å¯èƒ½åŸå› **ï¼š
- Padding åˆ°æœ€å¤§å°ºå¯¸å¯¼è‡´æ˜¾å­˜æµªè´¹

**è§£å†³æ–¹æ³•**ï¼š
- å‡å° batch_size
- é™åˆ¶ `multi_resolutions` çš„èŒƒå›´
- é…ç½® `max_aspect_ratio` é¿å…æç«¯å°ºå¯¸

#### Q3: å•æ ·æœ¬æ¨ç†ç»“æœä¸ä¹‹å‰ä¸ä¸€è‡´

**å¯èƒ½åŸå› **ï¼š
- `_should_use_multi_resolution_mode` åˆ¤æ–­é”™è¯¯
- å•æ ·æœ¬èµ°äº†å¤šåˆ†è¾¨ç‡è·¯å¾„

**è§£å†³æ–¹æ³•**ï¼š
```python
# ç¡®è®¤å•æ ·æœ¬ä½¿ç”¨ shared mode
if batch_size == 1:
    assert not self._should_use_multi_resolution_mode(batch)
```

### 9.4 æ€§èƒ½ç›‘æ§å»ºè®®

è™½ç„¶æ–‡æ¡£ä¸åŒ…å«æ€§èƒ½ä¼˜åŒ–ç­–ç•¥ï¼Œä½†å»ºè®®è®°å½•ä»¥ä¸‹æŒ‡æ ‡ç”¨äºç›‘æ§ï¼š

```python
# åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ 
padding_ratio = (total_tokens - valid_tokens) / total_tokens
logging.info(f"Padding ratio: {padding_ratio:.2%}")

# æ¯ä¸ª epoch ç»Ÿè®¡
avg_padding = sum(padding_ratios) / len(padding_ratios)
if avg_padding > 0.3:
    logging.warning(f"High average padding ratio: {avg_padding:.2%}")
```

### 9.5 Rollback æ–¹æ¡ˆ

å¦‚æœå¤šåˆ†è¾¨ç‡è®­ç»ƒå‡ºç°é—®é¢˜ï¼Œå¯ä»¥å¿«é€Ÿå›é€€åˆ°å•åˆ†è¾¨ç‡ï¼š

```yaml
# æ–¹å¼ 1ï¼šç§»é™¤ multi_resolutionsï¼Œä½¿ç”¨å›ºå®šå°ºå¯¸
data:
  init_args:
    processor:
      init_args:
        target_size: [512, 512]  # å›åˆ°å›ºå®šå°ºå¯¸

# æ–¹å¼ 2ï¼šæ”¹ç”¨å•ä¸€é¢ç§¯
data:
  init_args:
    processor:
      init_args:
        target_pixels: 262144  # 512*512ï¼Œä¸ä½¿ç”¨ candidates
```

---

## é™„å½•ï¼šå…³é”®è®¾è®¡å†³ç­–

### A0. é…ç½®è®¾è®¡ç®€åŒ–

**å†³ç­–**ï¼šå­˜åœ¨ `multi_resolutions` å³è‡ªåŠ¨å¯ç”¨å¤šåˆ†è¾¨ç‡ï¼Œæ— éœ€é¢å¤–å¼€å…³ã€‚

**ç†ç”±**ï¼š
- é¿å…é…ç½®å†—ä½™ï¼ˆ`multi_resolutions` + `multi_resolution.enabled`ï¼‰
- é…ç½®æ„å›¾æ˜ç¡®ï¼šæœ‰å€™é€‰åˆ—è¡¨ = å¤šåˆ†è¾¨ç‡ï¼›æ— å€™é€‰åˆ—è¡¨ = å•åˆ†è¾¨ç‡
- ç®€åŒ–åˆ¤æ–­é€»è¾‘ï¼Œå‡å°‘é…ç½®é”™è¯¯ï¼ˆå¦‚å€™é€‰åˆ—è¡¨å­˜åœ¨ä½† enabled=false çš„çŸ›ç›¾æƒ…å†µï¼‰
- å‘åå…¼å®¹ï¼šæ—§é…ç½®ï¼ˆ`target_size` æˆ– `target_pixels`ï¼‰è‡ªåŠ¨ä½¿ç”¨å•åˆ†è¾¨ç‡æ¨¡å¼

### A1. ä¸ºä»€ä¹ˆä¸ä½¿ç”¨ Bucketingï¼Ÿ

æŒ‰ç”¨æˆ·è¦æ±‚ï¼Œä¸ä½¿ç”¨ bucketing ç­–ç•¥ã€‚ä¸»è¦è€ƒè™‘ï¼š
- å®ç°å¤æ‚åº¦è¾ƒé«˜
- å¯èƒ½å½±å“æ•°æ®éšæœºæ€§
- Padding æ–¹æ¡ˆæ›´é€šç”¨

### A2. ä¸ºä»€ä¹ˆéœ€è¦ Per-sample RoPEï¼Ÿ

RoPEï¼ˆRotary Position Embeddingï¼‰ä¾èµ–åºåˆ—çš„ç©ºé—´ç»“æ„ï¼ˆheight, widthï¼‰ã€‚ä¸åŒå°ºå¯¸çš„æ ·æœ¬å¿…é¡»ä½¿ç”¨å„è‡ªçš„ä½ç½®ç¼–ç ï¼Œå¦åˆ™ä½ç½®ä¿¡æ¯ä¼šé”™ä¹±ã€‚

### A3. Loss å½’ä¸€åŒ–çš„å¿…è¦æ€§

å‡è®¾ batch ä¸­æœ‰ä¸¤ä¸ªæ ·æœ¬ï¼š
- æ ·æœ¬ Aï¼š512x512ï¼Œæœ‰æ•ˆ token = 4096
- æ ·æœ¬ Bï¼š1024x1024ï¼Œpadding åˆ° 16384

å¦‚æœä¸æŒ‰æœ‰æ•ˆ token å½’ä¸€åŒ–ï¼š
```python
loss = (pred - target).pow(2).mean()  # é”™è¯¯ï¼
# æ ·æœ¬ A çš„ loss è¢«ç¨€é‡Šä¸º 4096/16384 = 25%
```

æ­£ç¡®åšæ³•ï¼š
```python
loss = ((pred - target).pow(2) * valid_mask).sum() / valid_mask.sum()
# æ ·æœ¬ A å’Œ B çš„ loss æƒé‡ç›¸åŒ
```

---

æ­¤æ–‡æ¡£è¯¦ç»†è¡¥å……äº†å¤šåˆ†è¾¨ç‡è®­ç»ƒçš„å®ç°ç»†èŠ‚ï¼ŒåŒ…æ‹¬æ•°æ®æ ¼å¼ã€è¾¹ç•Œæƒ…å†µã€ç¼“å­˜å…¼å®¹æ€§ã€per-sample RoPEã€æµ‹è¯•è®¡åˆ’å’Œè¿ç§»æŒ‡å—ï¼Œä¸ºåç»­å®æ–½æä¾›å®Œæ•´çš„æŠ€æœ¯è§„èŒƒã€‚
