trainer: DreamOmni2

model:
  pretrained_model_name_or_path: "black-forest-labs/FLUX.1-Kontext-dev"
  # pretrained_model_name_or_path: lrzjason/flux-kontext-nf4
  use_vlm_prompt_enhancer: true  # Enable VLM prompt enhancement
  lora:
    r: 16
    lora_alpha: 16
    init_lora_weights: gaussian
    target_modules: ["to_k", "to_q", "to_v", "to_out.0"]
    pretrained_weight: null
    adapter_name: "edit"

data:
  class_path: qflux.data.dataset.ImageDataset
  init_args:
    dataset_path:
      - split: train
        repo_id: TsienDragon/face_segmentation_20
    caption_dropout_rate: 0.0
    prompt_image_dropout_rate: 0.0
    use_edit_mask: true  # if true, dataset output edit mask
    selected_control_indexes: [1]
    processor:
      class_path: qflux.data.preprocess.ImageProcessor
      init_args:
        process_type: center_crop
        resize_mode: bilinear
        target_size: [832, 576]
        controls_size: [[832, 576]]

  batch_size: 2  # adjust batch size according to the available memory, can be set to 1, 2, 4
  num_workers: 2
  shuffle: true


validation:
  enabled: true
  steps: 10
  max_samples: 2
  seed: 42
  dataset:
    class_path: qflux.data.dataset.ImageDataset
    init_args:
      dataset_path:
        - split: test
          repo_id: TsienDragon/face_segmentation_20
      caption_dropout_rate: 0.0
      prompt_image_dropout_rate: 0.0
      use_edit_mask: true
      selected_control_indexes: [1]
      processor:
        class_path: qflux.data.preprocess.ImageProcessor
        init_args:
          process_type: center_crop
          resize_mode: bilinear
          target_size: [832, 576]
          controls_size: [[832, 576]]


logging:
  output_dir: /tmp/image_edit_lora/  # change the path to your own output path
  # report_to: tensorboard
  report_to: wandb
  # report_to: swanlab
  tracker_project_name: DreamOmni2Fp16
  tags:
    - test
    - DreamOmni2
    - FaceSeg
    - lilong
  notes: "This is a test configuration for DreamOmni2 on FaceSeg dataset"




optimizer:
  # class_path: bitsandbytes.optim.Adam8bit  # 8bit Adam optimizer to save memory
  # init_args:
  #   lr: 0.0001  # face segmentation task uses smaller learning rate
  #   betas: [0.9, 0.999]
  class_path:  prodigyopt.Prodigy # bitsandbytes.optim.Adam8bit  # 8bit Adam optimizer to save memory
  init_args:
    lr: 1.0  # face segmentation task uses smaller learning rate
    use_bias_correction: True
    safeguard_warmup: True
    weight_decay: 0.01

lr_scheduler:
  scheduler_type: "cosine"  # cosine scheduler, better for fine-grained tasks
  warmup_steps: 50  # increase warmup steps
  num_cycles: 0.5
  power: 1.0

loss:
  class_path: qflux.losses.AttentionMaskMseLoss  # Use attention mask loss for multi-resolution
  init_args:
    foreground_weight: 2.0
    background_weight: 1.0
    eps: 1e-12
    reduction: mean

train:
  gradient_accumulation_steps: 1  # increase gradient accumulation to simulate larger batch size
  max_train_steps: 10000  # face segmentation data is small, reduce training steps
  checkpointing_steps: 100  # save checkpoints more frequently
  max_grad_norm: 1.0
  mixed_precision: "bf16"
  gradient_checkpointing: True  # enable gradient checkpointing to save memory
  low_memory: True  # is used low_memory mode, then the model will be loaded on the specified devices
  # otherwise, the model will be loaded on all the gpus
  fit_device:
    vae: cuda:1
    text_encoder: cuda:1

cache:
  cache_dir: ${logging.output_dir}/${logging.tracker_project_name}/cache
  use_cache: true
  prompt_empty_drop_keys:
    - prompt_embeds
    - pooled_prompt_embeds
  devices:
    vae: "cuda:2"
    text_encoder: "cuda:2"
    text_encoder_2: "cuda:2"
    prompt_enhancer: "cuda:2"  # VLM model device during cache stage

predict:
  devices:
    vae: "cuda:2"
    text_encoder: "cuda:2"
    text_encoder_2: "cuda:2"
    dit: "cuda:2"
    prompt_enhancer: "cuda:2"  # VLM model device during predict stage
