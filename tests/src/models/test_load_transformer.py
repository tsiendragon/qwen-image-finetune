"""
æµ‹è¯• load_transformer å‡½æ•°
passed: 2025-10-22 10:00:00
"""

import pytest
import torch
from diffusers.pipelines.qwenimage.pipeline_qwenimage_edit import QwenImageEditPipeline

from qflux.models.load_model import load_transformer


def compare_models(model1, model2, model_name="models", tolerance=1e-6):
    """
    æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹æ˜¯å¦ç›¸åŒ

    Args:
        model1: ç¬¬ä¸€ä¸ªæ¨¡å‹
        model2: ç¬¬äºŒä¸ªæ¨¡å‹
        model_name: æ¨¡å‹åç§°ï¼Œç”¨äºæ—¥å¿—è¾“å‡º
        tolerance: å‚æ•°å€¼å·®å¼‚çš„å®¹å¿åº¦

    Returns:
        bool: å¦‚æœæ¨¡å‹å®Œå…¨ç›¸åŒè¿”å› Trueï¼Œå¦åˆ™è¿”å› False
    """
    print(f"ğŸ” æ¯”è¾ƒ {model_name}...")

    # 1. æ£€æŸ¥æ¨¡å‹ç±»å‹
    print(f"Model 1 ç±»å‹: {type(model1)}")
    print(f"Model 2 ç±»å‹: {type(model2)}")

    # 2. æ£€æŸ¥æ¨¡å‹ç»“æ„
    param_count1 = sum(p.numel() for p in model1.parameters())
    param_count2 = sum(p.numel() for p in model2.parameters())
    print(f"\nModel 1 å‚æ•°æ•°é‡: {param_count1:,}")
    print(f"Model 2 å‚æ•°æ•°é‡: {param_count2:,}")

    assert param_count1 == param_count2, f"å‚æ•°æ•°é‡ä¸åŒ: {param_count1:,} vs {param_count2:,}"

    # 3. æ£€æŸ¥state_dicté”®
    state_dict1 = model1.state_dict()
    state_dict2 = model2.state_dict()

    keys1 = set(state_dict1.keys())
    keys2 = set(state_dict2.keys())

    print(f"\nModel 1 å‚æ•°é”®æ•°é‡: {len(keys1)}")
    print(f"Model 2 å‚æ•°é”®æ•°é‡: {len(keys2)}")

    # æ£€æŸ¥é”®æ˜¯å¦å®Œå…¨ç›¸åŒ
    only_in_1 = keys1 - keys2
    only_in_2 = keys2 - keys1

    if only_in_1:
        print(f"âŒ Model 1 ç‹¬æœ‰çš„é”®: {only_in_1}")
    if only_in_2:
        print(f"âŒ Model 2 ç‹¬æœ‰çš„é”®: {only_in_2}")

    assert keys1 == keys2, "å‚æ•°é”®ä¸å®Œå…¨ç›¸åŒï¼"
    print("âœ… å‚æ•°é”®å®Œå…¨ç›¸åŒ")

    # 4. æ£€æŸ¥å‚æ•°å€¼æ˜¯å¦ç›¸åŒ
    print("\nğŸ” æ£€æŸ¥å‚æ•°å€¼...")
    differences = 0
    max_diff = 0.0
    diff_details = []

    for key in keys1:
        param1 = state_dict1[key]
        param2 = state_dict2[key]

        # æ£€æŸ¥å½¢çŠ¶
        if param1.shape != param2.shape:
            print(f"âŒ å‚æ•° {key} çš„å½¢çŠ¶ä¸åŒ: {param1.shape} vs {param2.shape}")
            differences += 1
            continue

        # æ£€æŸ¥å€¼
        diff = torch.abs(param1 - param2).max().item()
        max_diff = max(max_diff, diff)

        if diff > tolerance:
            print(f"âš ï¸  å‚æ•° {key} æœ‰å·®å¼‚: æœ€å¤§å·®å€¼ = {diff}")
            differences += 1
            diff_details.append((key, diff))

    print("\nğŸ“Š æ¯”è¾ƒç»“æœ:")
    print(f"å‚æ•°å·®å¼‚æ•°é‡: {differences}")
    print(f"æœ€å¤§å·®å€¼: {max_diff}")

    if differences == 0:
        print("âœ… æ‰€æœ‰å‚æ•°å€¼å®Œå…¨ç›¸åŒï¼")
        return True
    else:
        print(f"âŒ å‘ç° {differences} ä¸ªå‚æ•°æœ‰å·®å¼‚")
        # æ˜¾ç¤ºå·®å¼‚æœ€å¤§çš„å‰5ä¸ªå‚æ•°
        if diff_details:
            diff_details.sort(key=lambda x: x[1], reverse=True)
            print("\nå·®å¼‚æœ€å¤§çš„å‚æ•°:")
            for key, diff in diff_details[:5]:
                print(f"  {key}: {diff}")
        return False


@pytest.mark.integration
def test_load_transformer_consistency():
    """
    æµ‹è¯• load_transformer å‡½æ•°æ˜¯å¦ä¸ QwenImageEditPipeline åŠ è½½çš„ transformer ä¸€è‡´

    æ¯”è¾ƒä¸¤ç§åŠ è½½æ–¹å¼:
    1. ä» QwenImageEditPipeline åŠ è½½
    2. ç›´æ¥ä½¿ç”¨ load_transformer åŠ è½½

    ç¡®ä¿ä¸¤ç§æ–¹å¼å¾—åˆ°çš„æ¨¡å‹å®Œå…¨ç›¸åŒï¼ˆç±»å‹ã€å‚æ•°æ•°é‡ã€å‚æ•°é”®ã€å‚æ•°å€¼ï¼‰
    """
    model_path = "Qwen/Qwen-Image-Edit"
    weight_dtype = torch.bfloat16

    # æ–¹æ³•1ï¼šä»QwenImageEditPipelineåŠ è½½
    print("\nğŸ“¥ æ–¹æ³•1: ä»QwenImageEditPipelineåŠ è½½...")
    pipe = QwenImageEditPipeline.from_pretrained(
        model_path,
        torch_dtype=weight_dtype
    )
    transformer1 = pipe.transformer
    print("âœ… æ–¹æ³•1åŠ è½½æˆåŠŸ")

    # æ–¹æ³•2ï¼šç›´æ¥åŠ è½½transformer
    print("\nğŸ“¥ æ–¹æ³•2: ç›´æ¥åŠ è½½transformer...")
    transformer2 = load_transformer(model_path, weight_dtype)
    print("âœ… æ–¹æ³•2åŠ è½½æˆåŠŸ")

    # æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹
    print("\n" + "="*50)
    is_same = compare_models(transformer1, transformer2, "transformers", tolerance=1e-6)
    print("="*50)

    # é¢å¤–ä¿¡æ¯
    print("\nğŸ“‹ é¢å¤–ä¿¡æ¯:")
    print(f"Pipeline transformer device: {transformer1.device}")
    print(f"Direct load transformer device: {transformer2.device}")
    print(f"Pipeline transformer dtype: {next(transformer1.parameters()).dtype}")
    print(f"Direct load transformer dtype: {next(transformer2.parameters()).dtype}")

    # æ£€æŸ¥é…ç½®
    if hasattr(transformer1, 'config') and hasattr(transformer2, 'config'):
        keys_to_compare = ['patch_size', 'in_channels', 'out_channels', 'num_layers', 'attention_head_dim', 'num_attention_heads', 'joint_attention_dim', 'guidance_embeds', 'axes_dims_rope']
        config1 = transformer1.config
        config2 = transformer2.config
        for key in keys_to_compare:
            assert config1[key] == config2[key], f"é…ç½® {key} ä¸åŒ: {config1[key]} vs {config2[key]}"
    # æœ€ç»ˆæ–­è¨€
    assert is_same, "ä¸¤ç§åŠ è½½æ–¹å¼å¾—åˆ°çš„transformeræ¨¡å‹ä¸å®Œå…¨ç›¸åŒ"
    print("ğŸ‰ ç»“è®º: ä¸¤ç§åŠ è½½æ–¹å¼å¾—åˆ°çš„transformeræ¨¡å‹å®Œå…¨ç›¸åŒï¼")


if __name__ == "__main__":
    """
    ç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶è¿›è¡Œæµ‹è¯•
    """
    test_load_transformer_consistency()
