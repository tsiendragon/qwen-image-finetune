"""
æµ‹è¯•å‡½æ•°ï¼šæ¯”è¾ƒæœ¬åœ°æƒé‡å’ŒHuggingFaceä»“åº“æƒé‡çš„å‚æ•°å·®å¼‚
ä½¿ç”¨FluxKontextLoraTraineråŠ è½½ä¸¤ç§ä¸åŒæ¥æºçš„æ¨¡å‹ï¼Œæ£€æŸ¥å‚æ•°æ˜¯å¦ç›¸ç­‰
"""

import torch
import pytest
import logging
import os
from typing import Dict, Any
from unittest.mock import Mock

from qflux.trainer.flux_kontext_trainer import FluxKontextLoraTrainer
from qflux.utils.model_compare import compare_model_parameters
from qflux.data.config import load_config_from_yaml

logger = logging.getLogger(__name__)


class TestModelWeightsComparison:
    """æµ‹è¯•æ¨¡å‹æƒé‡æ¯”è¾ƒåŠŸèƒ½"""

    def setup_method(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        # ä½¿ç”¨æŒ‡å®šçš„é…ç½®æ–‡ä»¶è·¯å¾„
        self.config_path = 'tests/test_configs/test_example_fluxkontext_fp16_character_composition.yaml'

    def create_config_with_lora_weight(self, lora_weight: str = None):
        """ä»YAMLé…ç½®æ–‡ä»¶åˆ›å»ºé…ç½®å¯¹è±¡å¹¶è®¾ç½®LoRAæƒé‡"""
        # ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®
        config = load_config_from_yaml(self.config_path)

        # è®¾ç½®LoRAæƒé‡
        if lora_weight:
            config.model.lora.pretrained_weight = lora_weight

        return config

    def compare_lora_weights(
        self,
        local_weights: str,
        repo_id: str,
        tolerance: float = 1e-6,
        verbose: bool = True
    ) -> Dict[str, Any]:
        """
        æ¯”è¾ƒæœ¬åœ°æƒé‡å’ŒHuggingFaceä»“åº“æƒé‡çš„å‚æ•°å·®å¼‚

        Args:
            local_weights: æœ¬åœ°æƒé‡æ–‡ä»¶è·¯å¾„
            repo_id: HuggingFaceä»“åº“ID
            tolerance: æ¯”è¾ƒå®¹å·®
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

        Returns:
            åŒ…å«æ¯”è¾ƒç»“æœçš„å­—å…¸
        """
        if verbose:
            print(f"\n{'='*80}")
            print("å¼€å§‹æ¯”è¾ƒæ¨¡å‹æƒé‡:")
            print(f"æœ¬åœ°æƒé‡: {local_weights}")
            print(f"HuggingFaceä»“åº“: {repo_id}")
            print(f"å®¹å·®: {tolerance}")
            print(f"{'='*80}")

        # æ£€æŸ¥æœ¬åœ°æƒé‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(local_weights):
            raise FileNotFoundError(f"æœ¬åœ°æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {local_weights}")

        try:
            # 1. åˆ›å»ºä½¿ç”¨æœ¬åœ°æƒé‡çš„trainer
            if verbose:
                print("\nğŸ“‚ åŠ è½½æœ¬åœ°æƒé‡æ¨¡å‹...")

            local_config = self.create_config_with_lora_weight(lora_weight=local_weights)
            local_trainer = FluxKontextLoraTrainer(local_config)

            # è®¾ç½®é¢„æµ‹æ¨¡å¼åŠ è½½æ¨¡å‹
            local_trainer.setup_predict()
            local_model = local_trainer.dit

            if verbose:
                print("âœ… æœ¬åœ°æƒé‡æ¨¡å‹åŠ è½½æˆåŠŸ")
                print(f"   æ¨¡å‹ç±»å‹: {type(local_model)}")
                print(f"   å‚æ•°æ•°é‡: {sum(p.numel() for p in local_model.parameters()):,}")

            # 2. åˆ›å»ºä½¿ç”¨HuggingFaceä»“åº“æƒé‡çš„trainer
            if verbose:
                print("\nğŸ¤— åŠ è½½HuggingFaceä»“åº“æƒé‡æ¨¡å‹...")

            hf_config = self.create_config_with_lora_weight(lora_weight=repo_id)
            hf_trainer = FluxKontextLoraTrainer(hf_config)

            # è®¾ç½®é¢„æµ‹æ¨¡å¼åŠ è½½æ¨¡å‹
            hf_trainer.setup_predict()
            hf_model = hf_trainer.dit

            if verbose:
                print("âœ… HuggingFaceæƒé‡æ¨¡å‹åŠ è½½æˆåŠŸ")
                print(f"   æ¨¡å‹ç±»å‹: {type(hf_model)}")
                print(f"   å‚æ•°æ•°é‡: {sum(p.numel() for p in hf_model.parameters()):,}")

            # 3. æ¯”è¾ƒæ¨¡å‹å‚æ•°
            if verbose:
                print("\nğŸ” å¼€å§‹æ¯”è¾ƒæ¨¡å‹å‚æ•°...")

            comparison_results = compare_model_parameters(
                model1=local_model,
                model2=hf_model,
                model1_name="æœ¬åœ°æƒé‡æ¨¡å‹",
                model2_name="HuggingFaceæƒé‡æ¨¡å‹",
                relative_threshold=tolerance,
                verbose=verbose
            )

            # 4. é¢å¤–çš„LoRAç‰¹å®šæ¯”è¾ƒ
            if verbose:
                print("\nğŸ¯ LoRAç‰¹å®šå‚æ•°åˆ†æ...")

            # æ£€æŸ¥LoRAå‚æ•°
            local_lora_params = self._extract_lora_parameters(local_model)
            hf_lora_params = self._extract_lora_parameters(hf_model)

            comparison_results['lora_analysis'] = self._compare_lora_parameters(
                local_lora_params, hf_lora_params, tolerance, verbose
            )

            # 5. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            if verbose:
                self._print_final_report(comparison_results, tolerance)

            return comparison_results

        except Exception as e:
            error_msg = f"æ¨¡å‹æƒé‡æ¯”è¾ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
            logger.error(error_msg)
            if verbose:
                print(f"âŒ {error_msg}")
            raise

    def _extract_lora_parameters(self, model: torch.nn.Module) -> Dict[str, torch.Tensor]:
        """æå–LoRAå‚æ•°"""
        lora_params = {}
        for name, param in model.named_parameters():
            if any(lora_key in name.lower() for lora_key in ['lora_a', 'lora_b', 'adapter']):
                lora_params[name] = param.detach().cpu()
        return lora_params

    def _compare_lora_parameters(
        self,
        local_params: Dict[str, torch.Tensor],
        hf_params: Dict[str, torch.Tensor],
        tolerance: float,
        verbose: bool
    ) -> Dict[str, Any]:
        """æ¯”è¾ƒLoRAç‰¹å®šå‚æ•°"""
        lora_results = {
            'total_local_lora_params': len(local_params),
            'total_hf_lora_params': len(hf_params),
            'common_lora_params': 0,
            'identical_lora_params': 0,
            'different_lora_params': [],
            'missing_lora_params': {
                'in_local': list(set(hf_params.keys()) - set(local_params.keys())),
                'in_hf': list(set(local_params.keys()) - set(hf_params.keys()))
            }
        }

        common_lora_keys = set(local_params.keys()).intersection(set(hf_params.keys()))
        lora_results['common_lora_params'] = len(common_lora_keys)
        if verbose:
            print(f"   æœ¬åœ°LoRAå‚æ•°: {len(local_params)}")
            print(f"   HF LoRAå‚æ•°: {len(hf_params)}")
            print(f"   å…±åŒLoRAå‚æ•°: {len(common_lora_keys)}")

        for key in sorted(common_lora_keys):
            local_param = local_params[key]
            hf_param = hf_params[key]

            if local_param.shape == hf_param.shape:
                # è®¡ç®—å·®å¼‚
                abs_diff = torch.abs(local_param - hf_param)
                max_diff = torch.max(abs_diff).item()
                mean_diff = torch.mean(abs_diff).item()

                if max_diff <= tolerance:
                    lora_results['identical_lora_params'] += 1
                    if verbose:
                        print(f"   âœ… {key}: ç›¸åŒ (max_diff={max_diff:.2e})")
                else:
                    lora_results['different_lora_params'].append({
                        'key': key,
                        'max_diff': max_diff,
                        'mean_diff': mean_diff
                    })
                    if verbose:
                        long_line = f"   âš ï¸  {key}: ä¸åŒ - "
                        long_line += f"max_diff={max_diff:.2e}, mean_diff={mean_diff:.2e}"
                        print(long_line)

        return lora_results

    def _print_final_report(self, results: Dict[str, Any], tolerance: float):
        """æ‰“å°æœ€ç»ˆæ¯”è¾ƒæŠ¥å‘Š"""
        print(f"\n{'='*80}")
        print(f"æœ€ç»ˆæ¯”è¾ƒæŠ¥å‘Š (å®¹å·®: {tolerance})")
        print(f"{'='*80}")

        # åŸºæœ¬ç»Ÿè®¡
        stats = results.get('statistics', {})
        print("ğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
        print(f"   æ€»å‚æ•°ç›¸åŒ: {stats.get('identical_params', 0)}")
        print(f"   æ€»å‚æ•°ä¸åŒ: {len(results.get('value_differences', []))}")
        print(f"   å½¢çŠ¶ä¸åŒ¹é…: {len(results.get('shape_differences', []))}")

        # LoRAç»Ÿè®¡
        lora_stats = results.get('lora_analysis', {})
        if lora_stats:
            print("\nğŸ¯ LoRAå‚æ•°ç»Ÿè®¡:")
            print(f"   LoRAå‚æ•°ç›¸åŒ: {lora_stats.get('identical_lora_params', 0)}")
            print(f"   LoRAå‚æ•°ä¸åŒ: {len(lora_stats.get('different_lora_params', []))}")

        # åˆ¤æ–­ç»“æœ
        total_differences = (len(results.get('value_differences', [])) +
                             len(results.get('shape_differences', [])))
        lora_differences = len(lora_stats.get('different_lora_params', []))

        if total_differences == 0 and lora_differences == 0:
            print("\nğŸ‰ ç»“è®º: æœ¬åœ°æƒé‡å’ŒHuggingFaceæƒé‡å®Œå…¨ç›¸åŒ!")
        elif lora_differences == 0:
            print("\nâœ… ç»“è®º: LoRAæƒé‡å®Œå…¨ç›¸åŒï¼ŒåŸºç¡€æ¨¡å‹å¯èƒ½æœ‰å·®å¼‚")
        else:
            print("\nâš ï¸  ç»“è®º: å‘ç°æƒé‡å·®å¼‚ï¼Œè¯·æ£€æŸ¥æ¨¡å‹ç‰ˆæœ¬æˆ–æƒé‡æ–‡ä»¶")

        print(f"{'='*80}")


def test_compare_lora_weights_equal():
    """æµ‹è¯•ç›¸åŒæƒé‡çš„æ¯”è¾ƒ - åº”è¯¥å®Œå…¨ç›¸ç­‰"""
    tester = TestModelWeightsComparison()

    # è¿™é‡Œéœ€è¦æä¾›å®é™…çš„æœ¬åœ°æƒé‡è·¯å¾„å’Œå¯¹åº”çš„HuggingFaceä»“åº“ID
    # ç¤ºä¾‹:
    base_path = '/tmp/image_edit_lora/character_composition_fp16/'
    weights_file = 'characterCompositionFluxKontextFp16/pytorch_lora_weights.safetensors'
    local_weights = base_path + weights_file
    repo_id = 'TsienDragon/qwen-image-edit-character-composition'

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è·³è¿‡æµ‹è¯•
    if not os.path.exists(local_weights):
        pytest.skip(f"æœ¬åœ°æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {local_weights}")

    try:
        results = tester.compare_lora_weights(
            local_weights=local_weights,
            repo_id=repo_id,
            tolerance=1e-6,
            verbose=True
        )

        # éªŒè¯ç»“æœ
        assert results is not None, "æ¯”è¾ƒç»“æœä¸èƒ½ä¸ºç©º"
        assert 'lora_analysis' in results, "ç¼ºå°‘LoRAåˆ†æç»“æœ"

        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜¾è‘—å·®å¼‚
        value_differences = results.get('value_differences', [])
        shape_differences = results.get('shape_differences', [])
        lora_differences = results['lora_analysis'].get('different_lora_params', [])

        total_diffs = len(value_differences) + len(shape_differences) + len(lora_differences)
        if total_diffs > 0:
            print("\nâš ï¸  å‘ç°æ¨¡å‹æƒé‡å·®å¼‚ï¼Œè¿™å¯èƒ½æ˜¯æ­£å¸¸çš„ï¼ˆä¸åŒçš„è®­ç»ƒè½®æ¬¡æˆ–ç‰ˆæœ¬ï¼‰")
        else:
            print("\nğŸ‰ æƒé‡å®Œå…¨ç›¸åŒ!")

    except Exception as e:
        pytest.fail(f"æµ‹è¯•å¤±è´¥: {str(e)}")


def test_compare_lora_weights_function():
    """ç‹¬ç«‹çš„æƒé‡æ¯”è¾ƒå‡½æ•°æµ‹è¯•"""

    def compare_lora_weights_standalone(local_weights: str, repo_id: str,
                                        tolerance: float = 1e-6) -> bool:
        """
        ç‹¬ç«‹çš„æƒé‡æ¯”è¾ƒå‡½æ•°

        Args:
            local_weights: æœ¬åœ°æƒé‡æ–‡ä»¶è·¯å¾„
            repo_id: HuggingFaceä»“åº“ID
            tolerance: æ¯”è¾ƒå®¹å·®

        Returns:
            bool: Trueè¡¨ç¤ºæƒé‡ç›¸ç­‰ï¼ŒFalseè¡¨ç¤ºä¸ç›¸ç­‰
        """
        tester = TestModelWeightsComparison()
        tester.setup_method()

        try:
            results = tester.compare_lora_weights(
                local_weights=local_weights,
                repo_id=repo_id,
                tolerance=tolerance,
                verbose=True
            )

            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ˜¾è‘—å·®å¼‚
            value_differences = results.get('value_differences', [])
            shape_differences = results.get('shape_differences', [])
            lora_analysis = results.get('lora_analysis', {})
            lora_differences = lora_analysis.get('different_lora_params', [])

            # æƒé‡ç›¸ç­‰çš„æ¡ä»¶ï¼šæ²¡æœ‰å€¼å·®å¼‚ã€å½¢çŠ¶å·®å¼‚å’ŒLoRAå·®å¼‚
            weights_equal = (
                len(value_differences) == 0 and
                len(shape_differences) == 0 and
                len(lora_differences) == 0
            )

            return weights_equal

        except Exception as e:
            logger.error(f"æƒé‡æ¯”è¾ƒå¤±è´¥: {str(e)}")
            return False

    # ç¤ºä¾‹ä½¿ç”¨
    base_path = '/tmp/image_edit_lora/character_composition_fp16/'
    weights_file = 'characterCompositionFluxKontextFp16/pytorch_lora_weights.safetensors'
    local_weights = base_path + weights_file
    repo_id = 'TsienDragon/qwen-image-edit-character-composition'

    if os.path.exists(local_weights):
        are_equal = compare_lora_weights_standalone(local_weights, repo_id)
        print(f"\næƒé‡æ¯”è¾ƒç»“æœ: {'ç›¸ç­‰' if are_equal else 'ä¸ç›¸ç­‰'}")
    else:
        print(f"æœ¬åœ°æƒé‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡æµ‹è¯•: {local_weights}")


def compare_weights_simple(local_weights: str, repo_id: str, tolerance: float = 1e-6) -> bool:
    """
    ç®€å•çš„æƒé‡æ¯”è¾ƒå‡½æ•°

    Args:
        local_weights: æœ¬åœ°æƒé‡æ–‡ä»¶è·¯å¾„
        repo_id: HuggingFaceä»“åº“ID
        tolerance: æ¯”è¾ƒå®¹å·®

    Returns:
        bool: Trueè¡¨ç¤ºæƒé‡ç›¸ç­‰ï¼ŒFalseè¡¨ç¤ºä¸ç›¸ç­‰
    """
    config_path = 'tests/test_configs/test_example_fluxkontext_fp16_character_composition.yaml'

    print(f"\n{'='*80}")
    print("å¼€å§‹æ¯”è¾ƒæ¨¡å‹æƒé‡:")
    print(f"æœ¬åœ°æƒé‡: {local_weights}")
    print(f"HuggingFaceä»“åº“: {repo_id}")
    print(f"é…ç½®æ–‡ä»¶: {config_path}")
    print(f"å®¹å·®: {tolerance}")
    print(f"{'='*80}")

    # æ£€æŸ¥æœ¬åœ°æƒé‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(local_weights):
        print(f"âŒ æœ¬åœ°æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {local_weights}")
        return False

    try:
        # 1. åŠ è½½æœ¬åœ°æƒé‡æ¨¡å‹
        print("\nğŸ“‚ åŠ è½½æœ¬åœ°æƒé‡æ¨¡å‹...")
        local_config = load_config_from_yaml(config_path)
        local_config.model.lora.pretrained_weight = local_weights
        local_trainer = FluxKontextLoraTrainer(local_config)
        local_trainer.setup_predict()
        local_model = local_trainer.dit

        print("âœ… æœ¬åœ°æƒé‡æ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"   æ¨¡å‹ç±»å‹: {type(local_model)}")
        print(f"   å‚æ•°æ•°é‡: {sum(p.numel() for p in local_model.parameters()):,}")

        # 2. åŠ è½½HuggingFaceæƒé‡æ¨¡å‹
        print("\nğŸ¤— åŠ è½½HuggingFaceä»“åº“æƒé‡æ¨¡å‹...")
        hf_config = load_config_from_yaml(config_path)
        hf_config.model.lora.pretrained_weight = repo_id
        hf_trainer = FluxKontextLoraTrainer(hf_config)
        hf_trainer.setup_predict()
        hf_model = hf_trainer.dit

        print("âœ… HuggingFaceæƒé‡æ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"   æ¨¡å‹ç±»å‹: {type(hf_model)}")
        print(f"   å‚æ•°æ•°é‡: {sum(p.numel() for p in hf_model.parameters()):,}")

        # 3. æ¯”è¾ƒæ¨¡å‹å‚æ•°
        print("\nğŸ” å¼€å§‹æ¯”è¾ƒæ¨¡å‹å‚æ•°...")

        local_state_dict = local_model.state_dict()
        hf_state_dict = hf_model.state_dict()

        local_keys = set(local_state_dict.keys())
        hf_keys = set(hf_state_dict.keys())
        common_keys = local_keys.intersection(hf_keys)

        print(f"æœ¬åœ°æ¨¡å‹å‚æ•°: {len(local_keys)}")
        print(f"HFæ¨¡å‹å‚æ•°: {len(hf_keys)}")
        print(f"å…±åŒå‚æ•°: {len(common_keys)}")

        if len(local_keys - hf_keys) > 0:
            print(f"æœ¬åœ°ç‹¬æœ‰å‚æ•°: {len(local_keys - hf_keys)}")
        if len(hf_keys - local_keys) > 0:
            print(f"HFç‹¬æœ‰å‚æ•°: {len(hf_keys - local_keys)}")

        # æ¯”è¾ƒå‚æ•°å€¼
        identical_params = 0
        different_params = 0
        shape_mismatches = 0

        for key in sorted(common_keys):
            local_param = local_state_dict[key]
            hf_param = hf_state_dict[key]

            # æ£€æŸ¥å½¢çŠ¶
            if local_param.shape != hf_param.shape:
                shape_mismatches += 1
                print(f"âŒ {key}: å½¢çŠ¶ä¸åŒ¹é… - {local_param.shape} vs {hf_param.shape}")
                continue

            # æ¯”è¾ƒå€¼
            try:
                local_param_cpu = local_param.detach().cpu().float()
                hf_param_cpu = hf_param.detach().cpu().float()

                abs_diff = torch.abs(local_param_cpu - hf_param_cpu)
                max_diff = torch.max(abs_diff).item()

                if max_diff <= tolerance:
                    identical_params += 1
                else:
                    different_params += 1
                    if different_params <= 10:  # åªæ˜¾ç¤ºå‰10ä¸ªä¸åŒçš„å‚æ•°
                        print(f"âš ï¸  {key}: ä¸åŒ - max_diff={max_diff:.2e}")
                    elif different_params == 11:
                        print("   ... (æ›´å¤šå·®å¼‚å‚æ•°æœªæ˜¾ç¤º)")

            except Exception as e:
                print(f"âŒ {key}: æ¯”è¾ƒæ—¶å‡ºé”™ - {e}")

        # ç»“æœç»Ÿè®¡
        print(f"\nğŸ“Š æ¯”è¾ƒç»“æœ:")
        print(f"   ç›¸åŒå‚æ•°: {identical_params}")
        print(f"   ä¸åŒå‚æ•°: {different_params}")
        print(f"   å½¢çŠ¶ä¸åŒ¹é…: {shape_mismatches}")

        # åˆ¤æ–­ç»“æœ
        weights_equal = (different_params == 0 and shape_mismatches == 0)

        if weights_equal:
            print("\nğŸ‰ ç»“è®º: æœ¬åœ°æƒé‡å’ŒHuggingFaceæƒé‡å®Œå…¨ç›¸åŒ!")
        else:
            print(f"\nâš ï¸  ç»“è®º: å‘ç°æƒé‡å·®å¼‚ - {different_params} ä¸ªå‚æ•°ä¸åŒï¼Œ{shape_mismatches} ä¸ªå½¢çŠ¶ä¸åŒ¹é…")

        print(f"{'='*80}")
        return weights_equal

    except Exception as e:
        error_msg = f"æ¨¡å‹æƒé‡æ¯”è¾ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        print(f"âŒ {error_msg}")
        return False


if __name__ == "__main__":
    # ç›´æ¥è¿è¡Œæµ‹è¯•
    print("è¿è¡Œæ¨¡å‹æƒé‡æ¯”è¾ƒæµ‹è¯•...")

    # ç¤ºä¾‹ä½¿ç”¨
    local_weights = '/tmp/image_edit_lora/character_composition_fp16/characterCompositionFluxKontextFp16/pytorch_lora_weights.safetensors'
    repo_id = 'TsienDragon/qwen-image-edit-character-composition'

    if os.path.exists(local_weights):
        are_equal = compare_weights_simple(local_weights, repo_id)
        print(f"\næœ€ç»ˆç»“æœ: æƒé‡{'ç›¸ç­‰' if are_equal else 'ä¸ç›¸ç­‰'}")
    else:
        print(f"æœ¬åœ°æƒé‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡æµ‹è¯•: {local_weights}")

    # å¯é€‰ï¼šè¿è¡Œpytestæµ‹è¯•
    # pytest -v tests/test_model_weights_comparison.py::test_compare_lora_weights_equal
