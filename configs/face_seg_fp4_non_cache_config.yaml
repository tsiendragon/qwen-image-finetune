model:
  # pretrained_model_name_or_path: "Qwen/Qwen-Image-Edit"
  pretrained_model_name_or_path: "ovedrive/qwen-image-edit-4bit"  # 4bit version
  rank: 16
  quantize: False
  lora:
    r: 16  # LoRA rank, can be adjusted(8, 16, 32), larger r means more parameters
    lora_alpha: 16  # LoRA alpha, usually equal to r
    init_lora_weights: "gaussian"
    target_modules: ["to_k", "to_q", "to_v", "to_out.0"]
    pretrained_weight: null

data:
  class_path: "qflux.data.dataset.ImageDataset"
  init_args:
    dataset_path: "/raid/lilong/data/face_seg/"  # change the path to your own dataset path
    image_size: [864, 1048]  # face segmentation usually uses 512x512
    caption_dropout_rate: 0.05  # reduce caption dropout, because the task is relatively specialized
    prompt_image_dropout_rate: 0.05
    cache_dir: ${cache.cache_dir}
    use_cache: ${cache.use_cache}
    cache_drop_rate: 0.1  # 10% probability of using empty prompt for training
    # crop configuration - use center crop instead of random crop
    random_crop: false  # disable random crop
    crop_size: 1024  # square size after cropping
    crop_scale: [0.8, 1.0]  # crop scale range
    center_crop: true  # enable center crop
    center_crop_ratio: 1.0  # center crop ratio, 0.9 means crop to 90% of the area
  batch_size: 2  # adjust batch size according to the available memory, can be set to 1, 2, 4
  num_workers: 2
  shuffle: true

logging:
  output_dir: "/raid/lilong/data/experiment/qwen-edit/"  # change the path to your own output path
  logging_dir: "logs"
  report_to: "tensorboard"
  tracker_project_name: "face_segmentation_lora"

optimizer:
  class_path: bitsandbytes.optim.Adam8bit  # 8bit Adam optimizer to save memory
  init_args:
    lr: 0.0001  # face segmentation task uses smaller learning rate
    betas: [0.9, 0.999]
  # if memory is sufficient, you can also use standard AdamW:
  # class_path: "torch.optim.AdamW"
  # init_args:
  #   lr: 0.0001
  #   weight_decay: 0.01
  #   betas: [0.9, 0.999]
  #   eps: 1e-8

lr_scheduler:
  scheduler_type: "cosine"  # cosine scheduler, better for fine-grained tasks
  warmup_steps: 50  # increase warmup steps
  num_cycles: 0.5
  power: 1.0

train:
  gradient_accumulation_steps: 1  # increase gradient accumulation to simulate larger batch size
  max_train_steps: 6000  # face segmentation data is small, reduce training steps
  num_epochs: 100  # increase epoch number
  checkpointing_steps: 100  # save checkpoints more frequently
  checkpoints_total_limit: 20
  max_grad_norm: 1.0
  mixed_precision: "bf16"
  gradient_checkpointing: True  # enable gradient checkpointing to save memory
  low_memory: True  # is used low_memory mode, then the model will be loaded on the specified devices
  # otherwise, the model will be loaded on all the gpus
  vae_encoder_device: cuda:0
  text_encoder_device: cuda:1

cache:
  vae_encoder_device: cuda:0
  text_encoder_device: cuda:0  # if there is only one card, put them on cuda:0
  cache_dir: "/raid/lilong/data/experiment/qwen-edit-face_seg_lora/cache"
  use_cache: False

predict:
  devices:
    vae: cuda:0
    text_encoder: cuda:0
    transformer: cuda:0

# training resume setting
resume_from_checkpoint: "latest"

# validation setting (optional)
validation:
  enabled: false  # if there is a validation set, enable it
  validation_steps: 200
  num_validation_samples: 4

# 22.17 GB used for traning with batchsize 2, speed 18.35s/it, on A100
