model:
  # pretrained_model_name_or_path: "Qwen/Qwen-Image-Edit"
  pretrained_model_name_or_path: "ovedrive/qwen-image-edit-4bit"  # 4bit
  # pretrained_model_name_or_path: "patientxtr/qwen-image-edit-fp8-e5m2"
  rank: 16
  quantize: False
  lora:
    r: 16
    lora_alpha: 16
    init_lora_weights: "gaussian"
    target_modules: ["to_k", "to_q", "to_v", "to_out.0"]
    adapter_name: "lora_edit"
    pretrained_weight: null

data:
  class_path: "src.data.dataset.ImageDataset"
  init_args:
    # 支持单个数据集路径
    # dataset_path: "/raid/lilong/data/kyc_gen/aligned/"
    # 支持多个数据集路径（推荐使用）
    dataset_path:
      - /data/lilong/ktp/ktp/dataset1/
      - /data/lilong/ktp/ktp/dataset1_crop/
    image_size: [832, 576]
    caption_dropout_rate: 0.1
    prompt_image_dropout_rate: 0.1
    cache_dir: ${cache.cache_dir}
    use_cache: ${cache.use_cache}
  batch_size: 4
  num_workers: 2
  shuffle: true

logging:
  output_dir: "/data/lilong/kyc_gen/logs/id_card_qwen_image_lora_inpainting/"
  logging_dir: "logs"
  report_to: "tensorboard"
  tracker_project_name: "lora_test3"

optimizer:
  class_path: bitsandbytes.optim.Adam8bit
  init_args:
    lr: 0.00005
    betas: [0.9, 0.999]
  # class_path: "torch.optim.AdamW"
  # init_args:
  #   lr: 0.0002
  #   weight_decay: 0.01
  #   betas: [0.9, 0.999]
  #   eps: 1e-8

lr_scheduler:
  scheduler_type: "constant_with_warmup"
  warmup_steps: 10
  num_cycles: 0.5
  power: 1.0

train:
  gradient_accumulation_steps: 1
  max_train_steps: 10000
  num_epochs: 100
  checkpointing_steps: 100
  checkpoints_total_limit: 50
  max_grad_norm: 1.0
  mixed_precision: "bf16"
  gradient_checkpointing: True  # 启用梯度检查点以节省显存
  resume_from_checkpoint: /data/lilong/kyc_gen/logs/id_card_qwen_image_lora_inpainting/lora_test3/v2/checkpoint-0-50

cache:
  vae_encoder_device: cuda:6
  text_encoder_device: cuda:7
  cache_dir: "/data/lilong/kyc_gen/logs/id_card_qwen_image_lora_inpainting/cache"
  use_cache: true

predict:
  devices:
    vae: cuda:5
    text_encoder: cuda:6
    transformer: cuda:7


