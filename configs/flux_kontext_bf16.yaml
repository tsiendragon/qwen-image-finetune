model:
  pretrained_model_name_or_path: "black-forest-labs/FLUX.1-Kontext-dev"
  model_type: "flux_kontext"
  quantize: false  # BF16 uses native precision
  lora:
    r: 16  # LoRA rank, can be adjusted (8, 16, 32)
    lora_alpha: 16  # LoRA alpha, usually equal to r
    init_lora_weights: "gaussian"
    target_modules: ["to_k", "to_q", "to_v", "to_out.0"]  # Flux transformer attention modules
    pretrained_weight: null

data:
  class_path: "src.data.dataset.ImageDataset"
  init_args:
    dataset_path: "/raid/lilong/data/face_seg/"  # change to your dataset path
    image_size: [1024, 1024]  # Flux Kontext typical resolution
    caption_dropout_rate: 0.05  # text prompt dropout for robustness
    prompt_image_dropout_rate: 0.05  # image prompt dropout
    cache_dir: ${cache.cache_dir}
    use_cache: ${cache.use_cache}
    cache_drop_rate: 0.1  # 10% probability of using empty prompt for training
    # Image processing configuration
    random_crop: false  # maintain aspect ratio for Flux
    crop_size: 1024  # square crop size
    crop_scale: [0.8, 1.0]  # crop scale range
    center_crop: true
    center_crop_ratio: 1.0
  batch_size: 1  # Flux models require smaller batches due to memory usage
  num_workers: 2
  shuffle: true

logging:
  output_dir: "/raid/lilong/data/experiment/flux-kontext/"  # change to your output path
  logging_dir: "logs"
  report_to: "tensorboard"
  tracker_project_name: "flux_kontext_bf16_lora"

optimizer:
  class_path: "torch.optim.AdamW"  # standard AdamW optimizer
  init_args:
    lr: 0.0001  # conservative learning rate for large models
    betas: [0.9, 0.999]
    weight_decay: 0.01
    eps: 1e-8

lr_scheduler:
  scheduler_type: "cosine"  # cosine annealing for stable training
  warmup_steps: 100  # warmup steps for large models
  num_cycles: 0.5
  power: 1.0

train:
  gradient_accumulation_steps: 8  # accumulate gradients due to small batch size
  max_train_steps: 5000  # adjust based on dataset size
  num_epochs: 50
  checkpointing_steps: 100
  checkpoints_total_limit: 10
  max_grad_norm: 1.0
  mixed_precision: "bf16"  # native BF16 precision
  gradient_checkpointing: true  # essential for memory efficiency
  low_memory: true
  vae_encoder_device: cuda:0
  text_encoder_device: cuda:1  # CLIP encoder
  text_encoder_2_device: cuda:1  # T5 encoder

cache:
  vae_encoder_device: cuda:0
  text_encoder_device: cuda:1  # CLIP encoder device
  text_encoder_2_device: cuda:1  # T5 encoder device
  cache_dir: "/raid/lilong/data/experiment/flux-kontext-bf16/cache"
  use_cache: true

predict:
  devices:
    vae: cuda:0
    text_encoder: cuda:1  # CLIP
    text_encoder_2: cuda:1  # T5
    transformer: cuda:0

# Training resume setting
resume_from_checkpoint: "latest"

# Validation setting (optional)
validation:
  enabled: false  # enable if validation set available
  validation_steps: 200
  num_validation_samples: 4

# Expected memory usage: ~24GB VRAM for training with batch_size=1
# Training speed: ~45s/it on A100 80GB
# Model quality: Highest, suitable for production use
